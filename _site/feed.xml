<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.6">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2020-01-16T11:30:22+01:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Akash Nair</title><subtitle>I am a DevOps Engineer and infrastructure nerd. I design and build on the cloud.</subtitle><author><name>Akash Nair</name></author><entry><title type="html">Jenkins, Kaniko, Kubernetes: The Cloud CI/CD Trifecta</title><link href="http://localhost:4000/jenkins-kaniko-kubernetes-cloud-ci-cd/" rel="alternate" type="text/html" title="Jenkins, Kaniko, Kubernetes: The Cloud CI/CD Trifecta" /><published>2019-06-24T00:00:00+02:00</published><updated>2019-06-24T00:00:00+02:00</updated><id>http://localhost:4000/jenkins-kaniko-kubernetes-the-cloud-ci-cd-trifecta</id><content type="html" xml:base="http://localhost:4000/jenkins-kaniko-kubernetes-cloud-ci-cd/">&lt;h3 id=&quot;introduction&quot;&gt;Introduction&lt;/h3&gt;
&lt;p&gt;Running Jenkins on Kubernetes unleashes the scalability powers of Jenkins and makes it easier to replicate and set it up. It involves running a Jenkins Master server (&lt;a href=&quot;https://hub.docker.com/r/jenkinsci/blueocean&quot;&gt;jenkinsci/blueocean&lt;/a&gt;) as a StatefulSet and connecting it with the Kubernetes cluster so that the master can spawn “jenkins slaves” on the K8S cluster whenever a build job is triggered. These slaves are nothing but K8S pods consisting of containers based on the &lt;a href=&quot;https://hub.docker.com/r/jenkinsci/jnlp-slave&quot;&gt;jenkinsci/jnlp-slave&lt;/a&gt; docker image from Docker Hub.&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;https://jenkins.io/images/jenkins-x/logo.svg&quot; height=&quot;400&quot; width=&quot;400&quot; /&gt;&lt;/center&gt;
&lt;!--more--&gt;

&lt;h3 id=&quot;step-i-create-and-deploy-jenkins-master&quot;&gt;Step I: Create and deploy Jenkins master&lt;/h3&gt;

&lt;p&gt;We first need to create a &lt;strong&gt;Dockerfile&lt;/strong&gt; for Jenkins master based on &lt;code class=&quot;highlighter-rouge&quot;&gt;jenkinsci/blueocean&lt;/code&gt; and install the &lt;strong&gt;Kuberentes plug-in&lt;/strong&gt; inside it along with any other optional plug-ins. Build this image and push it to a docker registry&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;Refer: &lt;a href=&quot;https://github.com/slashr/jenkins-on-kubernetes/blob/master/Dockerfile.jenkins.master&quot;&gt;Dockerfile.jenkins.master&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Next deploy the Jenkins master image on a pod on Kubernetes. Note that although you can create a normal kuberentes &lt;code class=&quot;highlighter-rouge&quot;&gt;Deployment&lt;/code&gt; for this, it is recommended that you create a &lt;code class=&quot;highlighter-rouge&quot;&gt;StatefulSet&lt;/code&gt; instead. This ensures the state and configuration of the pod is maintained. In addition to it being a StatefulSet a &lt;code class=&quot;highlighter-rouge&quot;&gt;PersistentVolume&lt;/code&gt; and a &lt;code class=&quot;highlighter-rouge&quot;&gt;PersistentVolumeClaim&lt;/code&gt; is required to maintain the data stored inside the Jenkins master.&lt;/p&gt;

&lt;p&gt;With the above setup, even if we terminate the pods running Jenkins master, the state and data will persist and the new pod will launch with the existing settings in place.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Refer to this manifest to understand how to setup a PV and PVC: &lt;a href=&quot;https://github.com/slashr/jenkins-on-kubernetes/blob/master/persistent-volume.yaml&quot;&gt;persistent-volume.yaml&lt;/a&gt; Make sure to replace &lt;code class=&quot;highlighter-rouge&quot;&gt;spec.awsElasticBlockStore.volumeID&lt;/code&gt; with your own EBS volume ID.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;Jenkins master deployment manifest: &lt;a href=&quot;https://github.com/slashr/jenkins-on-kubernetes/blob/master/jenkins-deploy.yml&quot;&gt;jenkins-deploy.yml&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;step-ii-connect-jenkins-with-the-kubernetes-cluster&quot;&gt;Step II: Connect Jenkins with the Kubernetes cluster&lt;/h3&gt;
&lt;p&gt;We will first create a service for the master Jenkins server and the slaves. Service type for master should be &lt;code class=&quot;highlighter-rouge&quot;&gt;LoadBalancer&lt;/code&gt; and for slaves should be &lt;code class=&quot;highlighter-rouge&quot;&gt;ClusterIP&lt;/code&gt;. The ports required on the master are 80 (optionally 443) and on the slaves port 50000 (default JNLP port for communication between master and the slave). Refer the following file: &lt;a href=&quot;https://github.com/slashr/jenkins-on-kubernetes/blob/master/jenkins-service.yml&quot;&gt;jenkins-service.yml&lt;/a&gt;
  Optionally, you can bind the Jenkins master ELB URL with a custom domain (jenkins.example.com) by create a A Record (Alias type) in Route 53.&lt;/p&gt;

&lt;p&gt;To provide Jenkins access to the Kubernetes cluster so that it can create pods for every build job, perform the following steps&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Click on &lt;strong&gt;Manage Jenkins&lt;/strong&gt; on the Jenkins homepage and then click on &lt;strong&gt;Configure System&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;Disable the workers on the master Jenkins pod &lt;strong&gt;# of executors&lt;/strong&gt; to 0. This ensures that build jobs are run solely on slaves and not on the master&lt;/li&gt;
  &lt;li&gt;Scroll down to the end of the page and click on &lt;strong&gt;Add a new cloud&lt;/strong&gt; and then click on Kubernetes.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Enter&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Name&lt;/strong&gt; for the cluster&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Kubernetes URL&lt;/strong&gt; E.g https://api.kube-cluster.example.com. You can find your API endpoint by running &lt;code class=&quot;highlighter-rouge&quot;&gt;kubectl cluster-info&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Kubernetes Namespace&lt;/strong&gt;: It’s the namespace where you deployed your Jenkins master pod&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Credentials&lt;/strong&gt;: Enter the username/password of your cluster (located in ~/.kube/config). Click on &lt;strong&gt;Test Connection&lt;/strong&gt; to see if the credentials are correct and Jenkins is able to connect to Kubernetes.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Jenkins URL&lt;/strong&gt;: Enter the endpoint of your Jenkins Master service (could be an ELB URL or a custom domain)&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Jenkins Tunnel&lt;/strong&gt;: Enter the endpoint of your Jenkins Slave service created earlier. It’s usually a cluster IP along with a port (usually 5000). It should look like 100.76.43.23:50000. The Jenkins master connects to the Kubernetes cluster using this endpoint.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Leave everything else as it is and then click on &lt;strong&gt;Save&lt;/strong&gt;.&lt;/p&gt;

&lt;h3 id=&quot;step-iii-write-the-jenkinsfile&quot;&gt;Step III: Write the Jenkinsfile&lt;/h3&gt;

&lt;p&gt;Next, we need to define the template for the Kubernetes pods that will build our application. This is best done from inside the Jenkinsfile. You can find the Jenkinsfile template here: &lt;a href=&quot;https://github.com/slashr/jenkins-on-kubernetes/blob/master/Jenkinsfile&quot;&gt;Jenkinsfile&lt;/a&gt; 
  As you can see, we have three containers running in the pod.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Kaniko&lt;/strong&gt;: For building our docker images and pushing them to our ECR registry&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;kubectl&lt;/strong&gt;: To deploy our application on Kubernetes&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;jnlp-slave&lt;/strong&gt;: The jenkins slave container although not defined in the Jenkinsfile is automatically created inside the pod by Jenkins master.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We have three volumes:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;kube-config&lt;/strong&gt;: It’s the ~/.kube/config file of your Kubernetes cluster and is mounted on the kubectl container for authentication to the K8S cluster&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;aws-secret&lt;/strong&gt;: It’s the ~/.aws/credentials file required by Kaniko to authenticate with ECR and publish the docker images&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;docker-config&lt;/strong&gt;: It’s a simple JSON file consisting of the ECR registry endpoint. Example: &lt;a href=&quot;https://github.com/slashr/jenkins-on-kubernetes/blob/master/docker-registry-config.yaml&quot;&gt;https://github.com/slashr/jenkins-on-kubernetes/blob/master/docker-registry-config.yaml&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;podlabel&lt;/strong&gt;: this is a variable we use to uniquely name the pods that will be spawned. If we don’t use it, then there are problems creating pods parallel for each job in the queue.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Note that for registries other an ECR such as GCR, you would need an additional “authentication token” inside this file.&lt;/p&gt;

&lt;h3 id=&quot;step-iv-create-a-pipeline&quot;&gt;Step IV: Create a pipeline&lt;/h3&gt;
&lt;p&gt;Now that Jenkins and Kubernetes setup is complete, we can create a Multibranch Pipeline and see the pods being spawned for builds in action! Simply click on &lt;strong&gt;New Item&lt;/strong&gt; on the Jenkins Homepage, enter a name for the project and click on &lt;strong&gt;Multibranch Pipeline&lt;/strong&gt; and click OK.&lt;/p&gt;

&lt;p&gt;Then click on &lt;strong&gt;Add Source&lt;/strong&gt; and your git repository. Click on Save and then Jenkins will start scanning your repository and adding all branches which contain a Jenkinsfile in the root directory.&lt;/p&gt;

&lt;p&gt;That’s it! You will see a few pods being spawned already for the new branches in the “jenkins” namespace. From now onwards, pods will be spawned on Kubernetes for every build and terminated after the build is finished&lt;/p&gt;</content><author><name>Akash</name></author><category term="jenkins" /><category term="kubernetes" /><category term="kaniko" /><category term="jenkinsfile" /><category term="dind" /><summary type="html">Introduction Running Jenkins on Kubernetes unleashes the scalability powers of Jenkins and makes it easier to replicate and set it up. It involves running a Jenkins Master server (jenkinsci/blueocean) as a StatefulSet and connecting it with the Kubernetes cluster so that the master can spawn “jenkins slaves” on the K8S cluster whenever a build job is triggered. These slaves are nothing but K8S pods consisting of containers based on the jenkinsci/jnlp-slave docker image from Docker Hub.</summary></entry><entry><title type="html">Suppressing Fires using Gas: An IoT Experiment</title><link href="http://localhost:4000/fire-suppression-using-smart-sensors-iot/" rel="alternate" type="text/html" title="Suppressing Fires using Gas: An IoT Experiment" /><published>2018-03-17T22:03:52+01:00</published><updated>2018-03-17T22:03:52+01:00</updated><id>http://localhost:4000/fire-suppression-using-smart-sensors-iot</id><content type="html" xml:base="http://localhost:4000/fire-suppression-using-smart-sensors-iot/">&lt;p&gt;An IoT experiment to create a gas based suppression system that can be used to extinguish fires inside water sensitive enclosures such as data centers. We make use of Arduino microcontrollers and several sensors to create this system.&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;/assets/images/arduino-logo.svg&quot; height=&quot;400&quot; width=&quot;400&quot; /&gt;&lt;/center&gt;
&lt;!--more--&gt;
&lt;hr /&gt;

&lt;h2 id=&quot;the-design&quot;&gt;The Design&lt;/h2&gt;

&lt;p&gt;A fully autonomous monitoring and suppression system that detects fire and extinguishes it using inert gases while also ensuring safety and avoiding danger at the same time. This system consists of the following functionalities:&lt;/p&gt;

&lt;ul style=&quot;list-style-type: square;&quot;&gt;
  &lt;li&gt;
    The detection of fire using real-time data from fire and temperature sensors
  &lt;/li&gt;
  &lt;li&gt;
    The detection of personnel inside the data center using feedback from presence&lt;br /&gt; sensors
  &lt;/li&gt;
  &lt;li&gt;
    The release of gas to suppress the fire and continuous monitoring based on real-time&lt;br /&gt; feedback from fire and temperature sensors
  &lt;/li&gt;
  &lt;li&gt;
    The detection of any leftover gas or fire using feedback from the fire, temperature&lt;br /&gt; and gas sensors
  &lt;/li&gt;
  &lt;li&gt;
    The interfacing of the system using an HMI to allow monitoring and manual intervention in case of a unexpected event of malfunction.
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;the-architecture&quot;&gt;The Architecture&lt;/h2&gt;

&lt;p&gt;The system consist of the following sensors and devices&lt;/p&gt;

&lt;table style=&quot;height: 171px; width: 488px;&quot;&gt;
  &lt;tr&gt;
    &lt;td style=&quot;width: 164px;&quot;&gt;
      Device
    &lt;/td&gt;
    
    &lt;td style=&quot;width: 310px;&quot;&gt;
      Purpose
    &lt;/td&gt;
  &lt;/tr&gt;
  
  &lt;tr&gt;
    &lt;td style=&quot;width: 164px;&quot;&gt;
      MQ2 Gas sensor
    &lt;/td&gt;
    
    &lt;td style=&quot;width: 310px;&quot;&gt;
      Detection of inert gas
    &lt;/td&gt;
  &lt;/tr&gt;
  
  &lt;tr&gt;
    &lt;td style=&quot;width: 164px;&quot;&gt;
      MQ2 temperature sensor
    &lt;/td&gt;
    
    &lt;td style=&quot;width: 310px;&quot;&gt;
      Detection of fire (measures temperature)
    &lt;/td&gt;
  &lt;/tr&gt;
  
  &lt;tr&gt;
    &lt;td style=&quot;width: 164px;&quot;&gt;
      Magideal IR Infrared Fire sensor
    &lt;/td&gt;
    
    &lt;td style=&quot;width: 310px;&quot;&gt;
      Detection of fire (identifies fire)
    &lt;/td&gt;
  &lt;/tr&gt;
  
  &lt;tr&gt;
    &lt;td style=&quot;width: 164px;&quot;&gt;
      L9110 fans
    &lt;/td&gt;
    
    &lt;td style=&quot;width: 310px;&quot;&gt;
      Exhaust and release of gas
    &lt;/td&gt;
  &lt;/tr&gt;
  
  &lt;tr&gt;
    &lt;td style=&quot;width: 164px;&quot;&gt;
      Movement sensor
    &lt;/td&gt;
    
    &lt;td style=&quot;width: 310px;&quot;&gt;
      Detection of personnel
    &lt;/td&gt;
  &lt;/tr&gt;
  
  &lt;tr&gt;
    &lt;td style=&quot;width: 164px;&quot;&gt;
      Arudino Nano
    &lt;/td&gt;
    
    &lt;td style=&quot;width: 310px;&quot;&gt;
      Controlling sensors (System Nano)
    &lt;/td&gt;
  &lt;/tr&gt;
  
  &lt;tr&gt;
    &lt;td style=&quot;width: 164px;&quot;&gt;
      Arduino Nano
    &lt;/td&gt;
    
    &lt;td style=&quot;width: 310px;&quot;&gt;
      Reading system status (HMI Nano)
    &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;the-tools&quot;&gt;The Tools&lt;/h2&gt;

&lt;p&gt;The algorithm was designed using the Arduino IDE in Embedded C.&lt;/p&gt;

&lt;p&gt;This was initial tested using Proteus which simulated the circuitboard.&lt;/p&gt;

&lt;p&gt;We used these &lt;a href=&quot;https://github.com/andresarmento/modbus-arduino&quot;&gt;external libraries&lt;/a&gt; for the configuration of Modbus protocol which was used to help SCADA communicate with micro-controller.&lt;/p&gt;

&lt;p&gt;We implemented the core system logic on the System Nano. Whereas, the SCADA&lt;/p&gt;

&lt;p&gt;system using the Modbus system was implemented on the HMI Nano.&lt;/p&gt;

&lt;p&gt;Using the HMI Nano, we continuously polled the pins of the System Nano to get the status of the devices and sensors.&lt;/p&gt;

&lt;p&gt;Dishonourable mention: Arduino Olimexino is a failed product. It took a lot of troubleshooting attempts before we found out that the microcontroller was sending incorrect signals to the devices (fans changed direction of rotation randomly!)&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;the-code&quot;&gt;The Code&lt;/h2&gt;

&lt;ul style=&quot;list-style-type: square;&quot;&gt;
  &lt;li&gt;
    &lt;a href=&quot;https://github.com/slashr/FireSuppressionIoT/blob/master/FORNANO/FORNANO.ino&quot;&gt;System Nano&lt;/a&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;a href=&quot;https://github.com/slashr/FireSuppressionIoT/blob/master/SCADA_NANO/SCADA_P_V3.ino/SCADA_P_V3.ino.ino&quot;&gt;HMI Nano&lt;/a&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;the-product&quot;&gt;The Product&lt;figure id=&quot;attachment_661&quot; style=&quot;width: 1024px&quot; class=&quot;wp-caption aligncenter&quot;&gt;&lt;/figure&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2018/03/setup.jpg&quot; alt=&quot;Arduino Nano Jr. Arduino Nano Sr., Humble Breadboard, Village Idiot Arduino Olimexino (rightfully abandoned), Whole lotta sensors&quot; /&gt; Starting from the left: Arduino Nano Jr, Arduino Nano Sr, Humble Breadboard, Village Idiot Arduino Olimexino (rightfully abandoned), Whole lotta sensors&lt;/p&gt;</content><author><name>Akash</name></author><category term="arduino" /><category term="IoT" /><category term="microcontroller" /><category term="sensors" /><summary type="html">An IoT experiment to create a gas based suppression system that can be used to extinguish fires inside water sensitive enclosures such as data centers. We make use of Arduino microcontrollers and several sensors to create this system.</summary></entry><entry><title type="html">Using Lambda to Turn Off EC2 Instances During Off Hours</title><link href="http://localhost:4000/using-lambda-turn-off-ec2-instances/" rel="alternate" type="text/html" title="Using Lambda to Turn Off EC2 Instances During Off Hours" /><published>2016-11-24T11:53:30+01:00</published><updated>2016-11-24T11:53:30+01:00</updated><id>http://localhost:4000/using-lambda-turn-off-ec2-instances</id><content type="html" xml:base="http://localhost:4000/using-lambda-turn-off-ec2-instances/">&lt;p&gt;A major part of your AWS bills comes from EC2 uptime and EBS storage use. In order to save on these costs, it is recommended to stop your instances whenever it is not required. Often times, your Development, Test or UAT environments aren’t used by developers during off-hours. Thus this would be a great time frame to temporarily stop the instances and save up on costs.&lt;/p&gt;

&lt;p&gt;In order to do this, we can make use of AWS Lambda, which lets you run your code or without setting up any servers. Since it charges based on the compute time taken up by the code, it is very cost efficient. A task running twice within a 24-hour cycle for typically less than 60 seconds with memory consumption up to 128MB (like the script in this post) typically costs around $0.008 per month.&lt;/p&gt;

&lt;p&gt;We will use the official AWS SDK for Python, boto3 to write our script and then run it using Lambda functions.
&lt;!--more--&gt;&lt;/p&gt;
&lt;hr /&gt;

&lt;h3 id=&quot;pre-requisites&quot;&gt;Pre-requisites&lt;/h3&gt;

&lt;p&gt;First things first, make sure your IAM user has:&lt;/p&gt;

&lt;p&gt;a) Access to AWS Lambda and create functions&lt;/p&gt;

&lt;p&gt;b) Full permissions to create Roles on IAM&lt;/p&gt;

&lt;p&gt;c) Access to CloudWatch&lt;/p&gt;

&lt;p&gt;d) Access to EC2 to tag instances and test the script&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;create-the-function&quot;&gt;Create the Function&lt;/h3&gt;

&lt;p&gt;Once the permissions are in place, get started with the following steps&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;h5 id=&quot;create-an-iam-role&quot;&gt;Create an IAM Role&lt;/h5&gt;

    &lt;p&gt;Create an IAM Role for Lamba to use. You can either create a custom role and give granular permissions. The permissions required are listing, stopping, starting EC2 instances, and listing, creating, modifying CloudWatch logs. Additional dependent permissions may be required so use AWS Policy Simulator to check if you have all the permissions required.&lt;/p&gt;

    &lt;p&gt;Or you can opt for an &lt;strong&gt;AWS Managed Policy&lt;/strong&gt;. Add &lt;strong&gt;AmazonEC2FullAccess&lt;/strong&gt; and &lt;strong&gt;CloudWatchLogsFullAccess&lt;/strong&gt; permissions to the role.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;h5 id=&quot;create-two-lambda-functions-for-starting-and-stopping-the-instances-using-the-two-scripts-below&quot;&gt;Create two Lambda functions, for starting and stopping the instances using the two scripts below.&lt;/h5&gt;

    &lt;ol&gt;
      &lt;li&gt;Refer to &lt;a href=&quot;https://docs.aws.amazon.com/lambda/latest/dg/get-started-create-function.html&quot;&gt;this guide&lt;/a&gt; to get a hang of how to create a simple Lambda function.&lt;/li&gt;
      &lt;li&gt;Click on &lt;strong&gt;Create a function&lt;/strong&gt; and then choose &lt;strong&gt;Author from scratch&lt;/strong&gt;.&lt;/li&gt;
      &lt;li&gt;Enter a suitable name for the function, change the &lt;strong&gt;Runtime&lt;/strong&gt; to Python.&lt;/li&gt;
      &lt;li&gt;Under &lt;strong&gt;Role,&lt;/strong&gt; select &lt;strong&gt;Create a custom role.&lt;/strong&gt; You will be taken to a new page to create the role. Create a policy and then come back to the Lambda page and click &lt;strong&gt;Create Function.&lt;/strong&gt;&lt;/li&gt;
      &lt;li&gt;Under &lt;strong&gt;Add Triggers&lt;/strong&gt;, choose &lt;strong&gt;CloudWatch Events.&lt;/strong&gt; Click on the newly created box to configure it. Under &lt;strong&gt;Schedule expression,&lt;/strong&gt;you can add the cron expression you want.&lt;br /&gt;
   For instance &lt;code class=&quot;highlighter-rouge&quot;&gt;cron&amp;lt;strong&amp;gt;(0 3 ? * MON-FRI *)&amp;lt;/strong&amp;gt;&lt;/code&gt;.
        &lt;blockquote&gt;
          &lt;p&gt;Note: As a weird caveat, AWS requires the cron expression to contain a ‘?’ in any one of the first four fields. So &lt;code class=&quot;highlighter-rouge&quot;&gt;cron(0 3 * * MON-FRI *)&lt;/code&gt; will not work but &lt;code class=&quot;highlighter-rouge&quot;&gt;cron(0 3 ? * MON-FRI *)&lt;/code&gt; will.&lt;/p&gt;
        &lt;/blockquote&gt;
      &lt;/li&gt;
      &lt;li&gt;Then click on the function box and copy paste the above code into the editor window.&lt;/li&gt;
      &lt;li&gt;Choose the &lt;strong&gt;No VPC&lt;/strong&gt; option under &lt;strong&gt;Network&lt;/strong&gt;. Keep your Lambda function inside a VPC when the function needs to login into an EC2 instance which is kept in a private subnet and is not publicly accessible. For actions that can be performed over the internet, it is recommended to not put the function inside a VPC. For example, boto’s start and stop API calls are publicly exposed and accessible over the internet which means that Lambda invoke them over the internet but not from within a VPC&lt;/li&gt;
      &lt;li&gt;Click on &lt;strong&gt;Save&lt;/strong&gt; and then click on &lt;strong&gt;Test&lt;/strong&gt; for a dry run.&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;You might want to run a few tests to ensure everything is working as expected. You don’t want your instance to be automatically shut down at 15:00 UTC (or maybe you do)&lt;/p&gt;</content><author><name>Akash</name></author><category term="automation" /><category term="cloudwatch" /><category term="devops" /><category term="ec2" /><summary type="html">A major part of your AWS bills comes from EC2 uptime and EBS storage use. In order to save on these costs, it is recommended to stop your instances whenever it is not required. Often times, your Development, Test or UAT environments aren’t used by developers during off-hours. Thus this would be a great time frame to temporarily stop the instances and save up on costs. In order to do this, we can make use of AWS Lambda, which lets you run your code or without setting up any servers. Since it charges based on the compute time taken up by the code, it is very cost efficient. A task running twice within a 24-hour cycle for typically less than 60 seconds with memory consumption up to 128MB (like the script in this post) typically costs around $0.008 per month. We will use the official AWS SDK for Python, boto3 to write our script and then run it using Lambda functions.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/wp-content/uploads/2016/11/aws-blackbelt-2015-aws-lambda-8-638.jpg" /><media:content medium="image" url="http://localhost:4000/wp-content/uploads/2016/11/aws-blackbelt-2015-aws-lambda-8-638.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">RDS Read Replica for Aurora using MySQL</title><link href="http://localhost:4000/rds-read-replica-aurora-mysql/" rel="alternate" type="text/html" title="RDS Read Replica for Aurora using MySQL" /><published>2016-09-07T11:03:15+02:00</published><updated>2016-09-07T11:03:15+02:00</updated><id>http://localhost:4000/rds-read-replica-aurora-mysql</id><content type="html" xml:base="http://localhost:4000/rds-read-replica-aurora-mysql/">&lt;p&gt;Default Read Replicas provided by AWS are instances which should be of the same instance type as the master DB or higher. This might not always be an ideal solution since read replica servers do not require significant computing power. As an alternative, one can create a read replica on a standalone EC2 instance or a RDS DB instance. In the following scenario, we’ll create a read replica running on MySQL syncing with a master DB running on Amazon Aurora. The replica instance type can be any type as seen fit by the user.
&lt;!--more--&gt;&lt;/p&gt;
&lt;hr /&gt;

&lt;h3 id=&quot;configure-master-db-for-replication&quot;&gt;Configure Master DB for replication&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;h4 id=&quot;enable-binary-logging-on-master&quot;&gt;Enable Binary Logging on master&lt;/h4&gt;

    &lt;ul&gt;
      &lt;li&gt;On the RDS Dashboard, click on &lt;strong&gt;Parameter Groups&lt;/strong&gt; on the left and then click on &lt;strong&gt;Create Parameter Group&lt;/strong&gt;. Under &lt;strong&gt;Type,&lt;/strong&gt; select DB Cluster Parameter Group. Enter a relevant &lt;strong&gt;Group Name&lt;/strong&gt; and &lt;strong&gt;Description&lt;/strong&gt; and click &lt;strong&gt;Create&lt;/strong&gt;.&lt;/li&gt;
      &lt;li&gt;Next, select the Parameter Group just created and click on &lt;strong&gt;Edit Parameters.&lt;/strong&gt; Then, under &lt;strong&gt;binlog_format&lt;/strong&gt;, change from &lt;strong&gt;OFF&lt;/strong&gt; to &lt;strong&gt;MIXED.&lt;/strong&gt;Click &lt;strong&gt;Save Changes&lt;/strong&gt;&lt;/li&gt;
      &lt;li&gt;Click on &lt;strong&gt;Clusters&lt;/strong&gt; on the RDS Dashboard, select the (master DB) cluster and click on &lt;strong&gt;Modify Cluster.&lt;/strong&gt; Under DB Cluster Parameter Group, select the Parameter Group just created. Click on &lt;strong&gt;Continue&lt;/strong&gt; and then on &lt;strong&gt;Modify Cluster.&lt;/strong&gt; The instance may reboot or become briefly inaccessible.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;h4 id=&quot;retain-binary-logs-set-retention-period&quot;&gt;Retain binary logs. Set retention period&lt;/h4&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;To do this, login to the master DB and run the following command:&lt;/p&gt;

        &lt;p&gt;&lt;code&gt; CALL mysql.rds_set_configuration('binlog retention hours', 144); &lt;/code&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Run &lt;code class=&quot;highlighter-rouge&quot;&gt;SHOW BINARY LOGS&lt;/code&gt; to ensure that logs are being retained.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;h4 id=&quot;create-snapshot-of-master-and-restore-from-snapshot&quot;&gt;Create snapshot of master and restore from snapshot.&lt;/h4&gt;
    &lt;ul&gt;
      &lt;li&gt;Create a snapshot of the master DB cluster. Restore from the snapshot a new RDS instance having the same parameter group as the master DB cluster (so that binary logging is enabled).&lt;/li&gt;
      &lt;li&gt;Connect to the newly created cluster and run &lt;code class=&quot;highlighter-rouge&quot;&gt;SHOW MASTER STATUS&lt;/code&gt; command. Retrieve the current binary log file name from the &lt;code&gt;File&lt;/code&gt; field and the log file position from the&lt;code&gt;Position&lt;/code&gt; field. Save these values for when you start replication.&lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Create a dump of the database. Use the following command:&lt;/p&gt;

        &lt;p&gt;&lt;code&gt; mysqldump -databases (database-name) -single-transaction -order-by-primary -r backup.sql -u (database-username) -host=(database-address) -p &lt;/code&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;After the dump is created, delete the restored DB cluster.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;configure-slave-db-for-replication&quot;&gt;Configure Slave DB for replication&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;Launch a MySQL RDS instance.&lt;/li&gt;
  &lt;li&gt;Connect to this MySQL RDS instance (slave DB) and restore the dump (source backup.sql)&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Enable replication by running the following queries (given in the &lt;a href=&quot;https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/AuroraMySQL.Replication.MySQL.html&quot;&gt;AWS documentation&lt;/a&gt;) on the master DB.&lt;/p&gt;

    &lt;p&gt;&lt;code&gt; 
        CALL mysql.rds_set_external_master (&quot;(master-db-endpoint)&quot;, 3306,&quot;(username)&quot;, &quot;(password)&quot;, &quot;(binary-log-file-name-from-step-3)&quot;, (log-file-position-from-step-3), 0); 
        CALL mysql.rds_start_replication;
        CALL mysql.rds_start_replication;
 &lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Finally, run SHOW SLAVE STATUS command on the replica and check &lt;strong&gt;Seconds behind master.&lt;/strong&gt; If the value is 0, it means there is no replica lag. When the replica lag is 0, reduce the retention period by setting the parameter binlog &lt;em&gt;retention hours&lt;/em&gt; to a smaller time frame.&lt;/li&gt;
&lt;/ol&gt;</content><author><name>Akash</name></author><category term="aurora" /><category term="aws" /><category term="devops" /><category term="mysql" /><category term="rds" /><summary type="html">Default Read Replicas provided by AWS are instances which should be of the same instance type as the master DB or higher. This might not always be an ideal solution since read replica servers do not require significant computing power. As an alternative, one can create a read replica on a standalone EC2 instance or a RDS DB instance. In the following scenario, we’ll create a read replica running on MySQL syncing with a master DB running on Amazon Aurora. The replica instance type can be any type as seen fit by the user.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/wp-content/uploads/2016/09/2000px-AWS_Simple_Icons_Database_Amazon_RDS_DB_Instance_Read_Replica.svg_.png" /><media:content medium="image" url="http://localhost:4000/wp-content/uploads/2016/09/2000px-AWS_Simple_Icons_Database_Amazon_RDS_DB_Instance_Read_Replica.svg_.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Docker: Install, Build an Image and Publish to DockerHub</title><link href="http://localhost:4000/docker/" rel="alternate" type="text/html" title="Docker: Install, Build an Image and Publish to DockerHub" /><published>2016-07-20T13:34:36+02:00</published><updated>2016-07-20T13:34:36+02:00</updated><id>http://localhost:4000/docker</id><content type="html" xml:base="http://localhost:4000/docker/">&lt;p&gt;Docker is a &lt;a href=&quot;https://en.wikipedia.org/wiki/Operating-system-level_virtualization&quot;&gt;container technology&lt;/a&gt; that provides a &lt;strong&gt;layer of abstraction&lt;/strong&gt; on the OS that it is installed on(Linux or Windows). It allows the packaging of a software application into a &lt;strong&gt;image&lt;/strong&gt; which consists of everything the software requires to run: code, system utilities, third party libraries and other dependencies. Then this image is run on the common &lt;strong&gt;layer of abstraction&lt;/strong&gt; defined above. Therefore, it becomes easy to move and deploy the software application (which refers to the &lt;strong&gt;container&lt;/strong&gt;) across any infrastructure or environment.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Updated: August, 2018&lt;/em&gt;&lt;/strong&gt;
&lt;!--more--&gt;&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;pre-requisites&quot;&gt;Pre-requisites:&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;Ubuntu 16.04 LTS 64-bit version. (Docker does not run on 32 bit)&lt;/li&gt;
  &lt;li&gt;Kernel version 3.10 and above. Check current kernel version using uname -r&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h4 id=&quot;installation&quot;&gt;Installation&lt;/h4&gt;

&lt;h5 id=&quot;the-easy-way&quot;&gt;The Easy Way&lt;/h5&gt;

&lt;p&gt;&lt;code&gt;wget -qO- https://get.docker.com/ | sh&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;OR&lt;/p&gt;

&lt;h5 id=&quot;the-longer-way-in-case-the-easy-way-doesnt-work-out&quot;&gt;The Longer Way (in case the easy way doesn’t work out)&lt;/h5&gt;

&lt;ol&gt;
  &lt;li&gt;Update APT sources &amp;amp; enable APT to work with HTTPS:
    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;code&gt;apt-get update&lt;/code&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;code&gt;sudo apt-get install apt-transport-https ca-certificates curl software-properties-common&lt;/code&gt;&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Add the GPG keys:
    &lt;ul&gt;
      &lt;li&gt;&lt;code&gt; curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - &lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Add the stable repository to the apt sources list:
    &lt;ul&gt;
      &lt;li&gt;&lt;code&gt;sudo add-apt-repository \   
 &quot;deb [arch=amd64] https://download.docker.com/linux/ubuntu \
 $(lsb_release -cs) \
 stable&quot;&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Update apt resource list:
    &lt;ul&gt;
      &lt;li&gt;&lt;code&gt;apt-get update&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Install Docker Community Edition
    &lt;ul&gt;
      &lt;li&gt;&lt;code&gt;apt-get install docker-ce&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;hr /&gt;

&lt;h4 id=&quot;running-docker&quot;&gt;Running Docker&lt;/h4&gt;

&lt;ol&gt;
  &lt;li&gt;Start Docker and test if it installed correctly. The second command will download the “hello-world” image form DockerHub, create a container and run it.
    &lt;ul&gt;
      &lt;li&gt;&lt;code&gt;service docker start&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;docker run hello-world&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;To view all the containers on the system, type:
    &lt;ul&gt;
      &lt;li&gt;&lt;code&gt;docker ps -a&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Try downloading and running the whalesay image from DockerHub. In order to do this, first create an account on &lt;a href=&quot;https://hub.docker.com/&quot;&gt;DockerHub&lt;/a&gt; and search for the image docker/whalesay. On the image page, you’ll find instructions on usage. For whalesay, use the following command
    &lt;ul&gt;
      &lt;li&gt;&lt;code&gt;docker run docker/whalesay cowsay boo&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;After the image downloads and runs, you’ll see the input in the form of a ASCII whale with a “boo” caption. Not very profound, I know. You can play around with the image by passing different parameters. Say:
    &lt;ul&gt;
      &lt;li&gt;&lt;code&gt;docker run docker/whalesay cowsay skywide!&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;In order to view all the images on the system, type:
    &lt;ul&gt;
      &lt;li&gt;&lt;code&gt;docker images&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;We’ll next learn how to build your own images. For now, use the above whalesay image and modify it to create our new image&lt;/li&gt;
&lt;/ol&gt;

&lt;hr /&gt;

&lt;h4 id=&quot;creating-your-own-docker-image&quot;&gt;Creating your own Docker image&lt;/h4&gt;

&lt;ol&gt;
  &lt;li&gt;Create a directory skywide.&lt;/li&gt;
  &lt;li&gt;CD into the directory&lt;/li&gt;
  &lt;li&gt;Create a file Dockerfile
    &lt;ul&gt;
      &lt;li&gt;&lt;code&gt;touch Dockerfile&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Open Dockerfile in an editor and add the following.:
    &lt;ul&gt;
      &lt;li&gt;&lt;code&gt;FROM: docker/whalesay:latest&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;RUN: apt-get -y update &amp;amp;&amp;amp; apt-get install -y fortunes&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;CMD: /usr/games/fortune -a | cowsay &lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;The FROM command tells Docker which image your image is based on. RUN command tells Docker to install the required programs/libraries on your image. CMD tells Docker to run the specified command when the image is run inside a container.&lt;/li&gt;
  &lt;li&gt;Next, build your image by running:
    &lt;ul&gt;
      &lt;li&gt;&lt;code&gt;docker build -t docker-whale .&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;The docker build command requires a Dockerfile to be present in the current directory. The “-t” option gives a name to the image being built (in this case, it is “docker-whale”)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Docker will build a new image by using the whalesay image that it downloaded earlier and on this new image it will update the apt-get cache and install the software fortunes.&lt;/li&gt;
  &lt;li&gt;You can then verify that a new image has been built by running docker images. You should now see three images including “docker-whale”.&lt;/li&gt;
  &lt;li&gt;Finally, see your own image in action! Go ahead and run:
    &lt;ul&gt;
      &lt;li&gt;&lt;code&gt;docker run docker-whale&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;hr /&gt;

&lt;h4 id=&quot;pushing-your-image-to-dockerhub&quot;&gt;Pushing your image to DockerHub&lt;/h4&gt;

&lt;ol&gt;
  &lt;li&gt;Create a new repository to upload your docker image to. Make sure the repository is public&lt;/li&gt;
  &lt;li&gt;Note down the image ID from
    &lt;ul&gt;
      &lt;li&gt;&lt;code&gt;docker images&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Tag your image with your username
    &lt;ul&gt;
      &lt;li&gt;&lt;code&gt;docker tag (image-id) (dockerhub-username) (repository-name):latest&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Type docker images again to see your new tagged image&lt;/li&gt;
  &lt;li&gt;You will need to login to Docker from the terminal before you can push it. To login
    &lt;ul&gt;
      &lt;li&gt;&lt;code&gt;docker login --username=(username) --email=(docker-email)&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;To push the image to your repo, type
    &lt;ul&gt;
      &lt;li&gt;&lt;code&gt;docker push (username)/(image-name)&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Finally, after the command finishes running, go to your DockerHub repository page to find your uploaded image.&lt;/li&gt;
&lt;/ol&gt;</content><author><name>Akash</name></author><category term="automation" /><category term="container" /><category term="devops" /><category term="docker" /><summary type="html">Docker is a container technology that provides a layer of abstraction on the OS that it is installed on(Linux or Windows). It allows the packaging of a software application into a image which consists of everything the software requires to run: code, system utilities, third party libraries and other dependencies. Then this image is run on the common layer of abstraction defined above. Therefore, it becomes easy to move and deploy the software application (which refers to the container) across any infrastructure or environment. Updated: August, 2018</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/wp-content/uploads/2016/07/large_v-trans.png" /><media:content medium="image" url="http://localhost:4000/wp-content/uploads/2016/07/large_v-trans.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Ansible Playbook to Create a Load-Balanced Tomcat Cluster</title><link href="http://localhost:4000/ansible-playbook-tomcat-cluster/" rel="alternate" type="text/html" title="Ansible Playbook to Create a Load-Balanced Tomcat Cluster" /><published>2016-07-13T09:35:26+02:00</published><updated>2016-07-13T09:35:26+02:00</updated><id>http://localhost:4000/ansible-playbook-tomcat-cluster</id><content type="html" xml:base="http://localhost:4000/ansible-playbook-tomcat-cluster/">&lt;p&gt;This Ansible playbook should give an idea about the configuration management capabilities of Ansible. It sets up a group of Tomcat nodes which are load-balanced by Apache using the Mod-jk module. In addition to this, it also sets up all other dependencies and also configures the installed services to run on startup.
&lt;!--more--&gt;
Overview of actions performed:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Update apt-get cache&lt;/li&gt;
  &lt;li&gt;Install Apache&lt;/li&gt;
  &lt;li&gt;Install Mod-jk module&lt;/li&gt;
  &lt;li&gt;Install MySQL&lt;/li&gt;
  &lt;li&gt;Install Java&lt;/li&gt;
  &lt;li&gt;Download Tomcat and setup nodes&lt;/li&gt;
  &lt;li&gt;Edit and configures Tomcat configuration files&lt;/li&gt;
  &lt;li&gt;Edit and configures Apache and Mod-jk configuration files&lt;/li&gt;
  &lt;li&gt;Restart Apache&lt;/li&gt;
  &lt;li&gt;Start Tomcat nodes&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Find the playbook in the repo here:
&lt;a href=&quot;https://github.com/slashr/Tomcat-Cluster-Load-Balancing&quot;&gt;https://github.com/slashr/Tomcat-Cluster-Load-Balancing&lt;/a&gt;&lt;/p&gt;</content><author><name>Akash</name></author><category term="ansible" /><category term="automation" /><category term="aws" /><category term="devops" /><category term="ec2" /><summary type="html">This Ansible playbook should give an idea about the configuration management capabilities of Ansible. It sets up a group of Tomcat nodes which are load-balanced by Apache using the Mod-jk module. In addition to this, it also sets up all other dependencies and also configures the installed services to run on startup.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/wp-content/uploads/2016/05/Ansible-Official-Logo-Black.png" /><media:content medium="image" url="http://localhost:4000/wp-content/uploads/2016/05/Ansible-Official-Logo-Black.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">IAM: Restrict EC2 Access to Specific Region</title><link href="http://localhost:4000/iam-restrict-ec2-access-specific-region/" rel="alternate" type="text/html" title="IAM: Restrict EC2 Access to Specific Region" /><published>2016-07-06T13:55:30+02:00</published><updated>2016-07-06T13:55:30+02:00</updated><id>http://localhost:4000/iam-restrict-ec2-access-specific-region</id><content type="html" xml:base="http://localhost:4000/iam-restrict-ec2-access-specific-region/">&lt;p&gt;Restrict user activity to a specific region in order to minimize resource wastage. This policy will ensure that all of the resources are being created only in the region of your choice so that when it comes down to resource cleanup and housekeeping, you’ll be able to save quite a lot of time.&lt;/p&gt;

&lt;p&gt;Apart from restricting access region-wise, it allows users read-only access to resources launched in other regions.&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/slashr/f17eb97ac86e94160ce03cc920373143.js&quot;&gt;&lt;/script&gt;</content><author><name>Akash</name></author><category term="aws" /><category term="ec2" /><category term="iam" /><summary type="html">Restrict user activity to a specific region in order to minimize resource wastage. This policy will ensure that all of the resources are being created only in the region of your choice so that when it comes down to resource cleanup and housekeeping, you’ll be able to save quite a lot of time. Apart from restricting access region-wise, it allows users read-only access to resources launched in other regions.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/wp-content/uploads/2016/07/iam-logo.png" /><media:content medium="image" url="http://localhost:4000/wp-content/uploads/2016/07/iam-logo.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Analyzing ELB Logs using Elasticsearch and Logstash</title><link href="http://localhost:4000/elb-logs-using-elk/" rel="alternate" type="text/html" title="Analyzing ELB Logs using Elasticsearch and Logstash" /><published>2016-06-16T08:50:16+02:00</published><updated>2016-06-16T08:50:16+02:00</updated><id>http://localhost:4000/elb-logs-using-elk</id><content type="html" xml:base="http://localhost:4000/elb-logs-using-elk/">&lt;p&gt;Elasticsearch, Logstash and Kibana (ELK) is a stack widely used for log analytics. The AWS Elasticsearch service lets you run the Elasticsearch and Kibana right out of the box with some manual setup of Logstash required. In this setup, we’ll walk through setting up the ELK stack on AWS, enabling ELB logs and analyzing the logs to identify and visualize trends like traffic spikes, frequently vistitor IPs etc.&lt;/p&gt;

&lt;hr /&gt;

&lt;h4 id=&quot;setting-up-elasticsearch&quot;&gt;Setting Up Elasticsearch&lt;/h4&gt;

&lt;ol&gt;
  &lt;li&gt;Choose a domain name that would be part of the ElasticSearch endpoint. For example, &lt;em&gt;mysite&lt;/em&gt;.&lt;/li&gt;
  &lt;li&gt;Select the number of instances in the cluster. Ideally select atleast 2 nodes so that the sharding works. Leave the rest of the options as it is and click &lt;strong&gt;Next&lt;/strong&gt;.&lt;/li&gt;
  &lt;li&gt;Under &lt;strong&gt;Set up access policy,&lt;/strong&gt; select &lt;strong&gt;Allow open access to the domain. &lt;/strong&gt;This will allow anyone to access your endpoint. Since we are only testing this, you can go ahead with this option. However, a production environment would require more restrictive access policies. Click &lt;strong&gt;Next&lt;/strong&gt; and then &lt;strong&gt;Confirm and Create. &lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;The domain will take around 5 to 10 minutes to get created.&lt;/li&gt;
  &lt;li&gt;Meanwhile, enable Access Logs on ELB by following &lt;a href=&quot;https://docs.aws.amazon.com/ElasticLoadBalancing/latest/DeveloperGuide/enable-access-logs.html&quot;&gt;this guide&lt;/a&gt;.&lt;/li&gt;
&lt;/ol&gt;

&lt;hr /&gt;

&lt;h4 id=&quot;setting-up-logstash&quot;&gt;Setting Up Logstash&lt;/h4&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Update and upgrade your system packages: 
 &lt;code class=&quot;highlighter-rouge&quot;&gt;sudo apt-get update &amp;amp;&amp;amp; sudo apt-get upgrade&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Logstash requires Java on the machine. Install Java: 
 &lt;code class=&quot;highlighter-rouge&quot;&gt;sudo apt-get install default-jdk&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Add Logstash Package Repositories to your sources list. Verify that you’re getting the latest repository info on &lt;a href=&quot;https://www.elastic.co/guide/en/logstash/current/installing-logstash.html#package-repositories&quot;&gt;this page&lt;/a&gt;&lt;/p&gt;

    &lt;p&gt;Run: &lt;code class=&quot;highlighter-rouge&quot;&gt;echo &quot;deb https://packages.elastic.co/logstash/2.3/debian stable main&quot; | sudo tee -a /etc/apt/sources.list&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Finally, update your packages again and install Logstash:
 &lt;code class=&quot;highlighter-rouge&quot;&gt;sudo apt-get update &amp;amp;&amp;amp; sudo apt-get install logstash&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Add your AWS IAM Access Key and Secret Key by using environment variables: 
 &lt;code class=&quot;highlighter-rouge&quot;&gt;echo AWS_ACCESS_KEY_ID=AKIAACCESSKEY&lt;/code&gt;&lt;/p&gt;

    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;echo AWS_SECRET_ACCESS_KEY=tUx2SECRETKEYACk32&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Create a Logstash configuration file and enter the Elasticsearch endpoint and S3 bucket name in it. Store this file as &lt;code class=&quot;highlighter-rouge&quot;&gt;/etc/logstash/conf.d/logstash.conf&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Run Logstash:
 &lt;code class=&quot;highlighter-rouge&quot;&gt;/opt/logstash/bin/logstash -f /etc/logstash/conf.d/logstash.conf&amp;lt;/code&amp;gt;&lt;/code&gt;&lt;/p&gt;
    &lt;hr /&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;setting-up-kibana&quot;&gt;Setting Up Kibana&lt;/h4&gt;

&lt;ol&gt;
  &lt;li&gt;Open Kibana from the URL provided on the Elasticsearch console.
    &lt;ul&gt;
      &lt;li&gt;Use the index pattern &lt;em&gt;elb_logs&lt;/em&gt;&lt;/li&gt;
      &lt;li&gt;Use the timestamp &lt;em&gt;@timestamp&lt;/em&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;To get started with visualizing your data, read through the &lt;a href=&quot;https://www.elastic.co/guide/en/kibana/current/visualize.html&quot;&gt;Visualize section&lt;/a&gt; of Elastic’s official documentation.&lt;/li&gt;
&lt;/ol&gt;</content><author><name>Akash</name></author><category term="analytics" /><category term="aws" /><category term="elasticsearch" /><category term="elb" /><category term="kibana" /><category term="logstash" /><summary type="html">Elasticsearch, Logstash and Kibana (ELK) is a stack widely used for log analytics. The AWS Elasticsearch service lets you run the Elasticsearch and Kibana right out of the box with some manual setup of Logstash required. In this setup, we’ll walk through setting up the ELK stack on AWS, enabling ELB logs and analyzing the logs to identify and visualize trends like traffic spikes, frequently vistitor IPs etc. Setting Up Elasticsearch Choose a domain name that would be part of the ElasticSearch endpoint. For example, mysite. Select the number of instances in the cluster. Ideally select atleast 2 nodes so that the sharding works. Leave the rest of the options as it is and click Next. Under Set up access policy, select Allow open access to the domain. This will allow anyone to access your endpoint. Since we are only testing this, you can go ahead with this option. However, a production environment would require more restrictive access policies. Click Next and then Confirm and Create.  The domain will take around 5 to 10 minutes to get created. Meanwhile, enable Access Logs on ELB by following this guide. Setting Up Logstash Update and upgrade your system packages: sudo apt-get update &amp;amp;&amp;amp; sudo apt-get upgrade Logstash requires Java on the machine. Install Java: sudo apt-get install default-jdk Add Logstash Package Repositories to your sources list. Verify that you’re getting the latest repository info on this page Run: echo &quot;deb https://packages.elastic.co/logstash/2.3/debian stable main&quot; | sudo tee -a /etc/apt/sources.list Finally, update your packages again and install Logstash: sudo apt-get update &amp;amp;&amp;amp; sudo apt-get install logstash Add your AWS IAM Access Key and Secret Key by using environment variables: echo AWS_ACCESS_KEY_ID=AKIAACCESSKEY echo AWS_SECRET_ACCESS_KEY=tUx2SECRETKEYACk32 Create a Logstash configuration file and enter the Elasticsearch endpoint and S3 bucket name in it. Store this file as /etc/logstash/conf.d/logstash.conf Run Logstash: /opt/logstash/bin/logstash -f /etc/logstash/conf.d/logstash.conf&amp;lt;/code&amp;gt; Setting Up Kibana Open Kibana from the URL provided on the Elasticsearch console. Use the index pattern elb_logs Use the timestamp @timestamp To get started with visualizing your data, read through the Visualize section of Elastic’s official documentation.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/wp-content/uploads/2016/06/LogstashElasticsearchKibana.png" /><media:content medium="image" url="http://localhost:4000/wp-content/uploads/2016/06/LogstashElasticsearchKibana.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Configuration Management on EC2 Using Ansible</title><link href="http://localhost:4000/configuration-management-ec2-ansible/" rel="alternate" type="text/html" title="Configuration Management on EC2 Using Ansible" /><published>2016-05-05T13:18:55+02:00</published><updated>2016-05-05T13:18:55+02:00</updated><id>http://localhost:4000/configuration-management-ec2-ansible</id><content type="html" xml:base="http://localhost:4000/configuration-management-ec2-ansible/">&lt;h4 id=&quot;introduction&quot;&gt;Introduction&lt;/h4&gt;

&lt;p&gt;Ansible is one of the easiest Configuration Management tools to get started with due to its client-less architecture. Ansible is written in Python and uses YAML for is configuration files which is a simple and human-readable syntax. One of the most important features of Ansible is its idempotency which means changes are only made when required and ignored otherwise. This prevents the possibility of inconsistency in your infrastructure.&lt;/p&gt;

&lt;hr /&gt;

&lt;h4 id=&quot;installation&quot;&gt;Installation&lt;/h4&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;The easiest and recommended way of installing is from the package repositories. Simply run: 
   &lt;code class=&quot;highlighter-rouge&quot;&gt;sudo apt-get install ansible&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;To test the installation, run the following. If you get a prompt asking for the SSH password, it indicates that the installation was successful. 
   &lt;code class=&quot;highlighter-rouge&quot;&gt;ansible all -m ping --ask-pass&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Open the file /etc/ansible/hosts and add the IPs of your servers in it. For example, you can add EC2 instance public IP or you can add the private IP if your control machine is the same VPC as the servers. The control machine as the name indicates is the machine which will be sending the commands to your servers.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;To test if the control machine is able to reach the servers, run the &lt;code class=&quot;highlighter-rouge&quot;&gt;ping&lt;/code&gt; command. Key-based authentication is encouraged over passwords. use the &lt;code class=&quot;highlighter-rouge&quot;&gt;--private-key&lt;/code&gt; flag to specify the path to your keyfile. If successful, it will return a JSON response containing the string &lt;code class=&quot;highlighter-rouge&quot;&gt;success&lt;/code&gt; 
   &lt;code class=&quot;highlighter-rouge&quot;&gt;ansible all -m ping --private-key ~/path/to/key -u ubuntu&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;To log in as root, add the become flag (-b)
   &lt;code class=&quot;highlighter-rouge&quot;&gt;ansible all -m ping --private-key ~/path/to/key -u ubuntu -b&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Test the configuration further by running a live command on the nodes
   &lt;code class=&quot;highlighter-rouge&quot;&gt;ansible all -a &quot;/bin/echo skywide&quot; --private-key ~/path/to/key -u ubuntu&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;If you’d like to have a fully automated and unattended setup, it would be wise to disable the host key checking feature. This feature when enabled will prompt for confirmation of the key the first time Ansible tries to log into the server. Disabling host key checking however has some implications and you might want to fully understand them before disabling the host check. Inside &lt;code class=&quot;highlighter-rouge&quot;&gt;/etc/ansible/ansible.cfg&lt;/code&gt;, edit the following &lt;code class=&quot;highlighter-rouge&quot;&gt;host_key_checking = False&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;This way, you are able to successfully run commands on your nodes. However, Ansible goes far beyond running simple commands on your nodes. To make the most of it, we need to make use of playbooks.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;hr /&gt;

&lt;h4 id=&quot;components&quot;&gt;Components&lt;/h4&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;h5 id=&quot;inventory&quot;&gt;Inventory&lt;/h5&gt;
    &lt;p&gt;The inventory file is nothing but a file containing the list of servers you want to configure. This file by default exists at /etc/ansible/hosts. It is also possible to use multiple inventory files.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;h5 id=&quot;dynamic-inventory&quot;&gt;Dynamic Inventory&lt;/h5&gt;
    &lt;p&gt;A Dynamic Inventory is when the inventory files are stored in a different system. For example, the files can be stored on a cloud storage provider, LDAP or other such storage solutions.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;h5 id=&quot;patterns&quot;&gt;Patterns&lt;/h5&gt;
    &lt;p&gt;The hosts that are specified in an inventory file can be grouped into categories like &lt;em&gt;web servers&lt;/em&gt; and &lt;em&gt;db servers&lt;/em&gt; by using the appropriate syntax. A Pattern refers to a set of groups (which are a set of hosts). Take for example, &lt;code class=&quot;highlighter-rouge&quot;&gt;ansible all -m ping&lt;/code&gt; Here &lt;em&gt;all&lt;/em&gt; refers to all hosts. Whereas in &lt;code class=&quot;highlighter-rouge&quot;&gt;ansible webservers -m ping&lt;/code&gt; the command will only work on the hosts in the group &lt;em&gt;webservers&lt;/em&gt; as defined in the inventory file.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;h5 id=&quot;variables&quot;&gt;Variables&lt;/h5&gt;
    &lt;p&gt;Variables, as the name suggests, are values that are used when you are working on systems that require varied configuration. Variables can be something like a port number or a private IP address. Variables can be defined either in Inventory files or Playbooks.
   Example:&lt;/p&gt;

    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;   - hosts: all
     vars:
       http_port: 80
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;

    &lt;p&gt;Here &lt;em&gt;http_port&lt;/em&gt; is a variable.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;h5 id=&quot;ad-hoc-commands&quot;&gt;Ad-Hoc Commands&lt;/h5&gt;
    &lt;p&gt;Ad-Hoc Commands are used to perform quick tasks that you don’t want to write an entire playbook for. Take the analogy of running a command through shell (ad-hoc command) and running it through a bash script (playbook). &lt;code class=&quot;highlighter-rouge&quot;&gt;ansible all -m ping&lt;/code&gt; is an example of a ad-hoc command.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;h5 id=&quot;playbooks&quot;&gt;Playbooks&lt;/h5&gt;
    &lt;p&gt;A playbook is a file containing instructions. The following playbook will install Apache web server on your nodes.&lt;/p&gt;

    &lt;p&gt;To run the playbook, use the following command:
   &lt;code class=&quot;highlighter-rouge&quot;&gt;ansible-playbook skywide.yaml --private-key ~/path/to/key.pem&lt;/code&gt;&lt;/p&gt;

    &lt;p&gt;If you want to avoid specifying the path to the key file every time, modify the file &lt;code class=&quot;highlighter-rouge&quot;&gt;/etc/ansible/ansible.cfg&lt;/code&gt; and set the path in the variable &lt;code class=&quot;highlighter-rouge&quot;&gt;private_key_file&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;</content><author><name>Akash</name></author><category term="ansible" /><category term="automation" /><category term="aws" /><category term="devops" /><category term="ec2" /><summary type="html">Introduction Ansible is one of the easiest Configuration Management tools to get started with due to its client-less architecture. Ansible is written in Python and uses YAML for is configuration files which is a simple and human-readable syntax. One of the most important features of Ansible is its idempotency which means changes are only made when required and ignored otherwise. This prevents the possibility of inconsistency in your infrastructure. Installation The easiest and recommended way of installing is from the package repositories. Simply run: sudo apt-get install ansible To test the installation, run the following. If you get a prompt asking for the SSH password, it indicates that the installation was successful. ansible all -m ping --ask-pass Open the file /etc/ansible/hosts and add the IPs of your servers in it. For example, you can add EC2 instance public IP or you can add the private IP if your control machine is the same VPC as the servers. The control machine as the name indicates is the machine which will be sending the commands to your servers. To test if the control machine is able to reach the servers, run the ping command. Key-based authentication is encouraged over passwords. use the --private-key flag to specify the path to your keyfile. If successful, it will return a JSON response containing the string success ansible all -m ping --private-key ~/path/to/key -u ubuntu To log in as root, add the become flag (-b) ansible all -m ping --private-key ~/path/to/key -u ubuntu -b Test the configuration further by running a live command on the nodes ansible all -a &quot;/bin/echo skywide&quot; --private-key ~/path/to/key -u ubuntu If you’d like to have a fully automated and unattended setup, it would be wise to disable the host key checking feature. This feature when enabled will prompt for confirmation of the key the first time Ansible tries to log into the server. Disabling host key checking however has some implications and you might want to fully understand them before disabling the host check. Inside /etc/ansible/ansible.cfg, edit the following host_key_checking = False This way, you are able to successfully run commands on your nodes. However, Ansible goes far beyond running simple commands on your nodes. To make the most of it, we need to make use of playbooks. Components Inventory The inventory file is nothing but a file containing the list of servers you want to configure. This file by default exists at /etc/ansible/hosts. It is also possible to use multiple inventory files. Dynamic Inventory A Dynamic Inventory is when the inventory files are stored in a different system. For example, the files can be stored on a cloud storage provider, LDAP or other such storage solutions. Patterns The hosts that are specified in an inventory file can be grouped into categories like web servers and db servers by using the appropriate syntax. A Pattern refers to a set of groups (which are a set of hosts). Take for example, ansible all -m ping Here all refers to all hosts. Whereas in ansible webservers -m ping the command will only work on the hosts in the group webservers as defined in the inventory file. Variables Variables, as the name suggests, are values that are used when you are working on systems that require varied configuration. Variables can be something like a port number or a private IP address. Variables can be defined either in Inventory files or Playbooks. Example: - hosts: all vars: http_port: 80 Here http_port is a variable. Ad-Hoc Commands Ad-Hoc Commands are used to perform quick tasks that you don’t want to write an entire playbook for. Take the analogy of running a command through shell (ad-hoc command) and running it through a bash script (playbook). ansible all -m ping is an example of a ad-hoc command. Playbooks A playbook is a file containing instructions. The following playbook will install Apache web server on your nodes. To run the playbook, use the following command: ansible-playbook skywide.yaml --private-key ~/path/to/key.pem If you want to avoid specifying the path to the key file every time, modify the file /etc/ansible/ansible.cfg and set the path in the variable private_key_file</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/wp-content/uploads/2016/05/Ansible-Official-Logo-Black.png" /><media:content medium="image" url="http://localhost:4000/wp-content/uploads/2016/05/Ansible-Official-Logo-Black.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">CloudFormation Template to Launch an EC2 instance</title><link href="http://localhost:4000/cloudformation-launch-ec2-execute-script-startup/" rel="alternate" type="text/html" title="CloudFormation Template to Launch an EC2 instance" /><published>2016-04-29T09:09:57+02:00</published><updated>2016-04-29T09:09:57+02:00</updated><id>http://localhost:4000/cloudformation-launch-ec2-execute-script-startup</id><content type="html" xml:base="http://localhost:4000/cloudformation-launch-ec2-execute-script-startup/">&lt;p&gt;The following is a bare-minimum CloudFormation template to launch an EC2 instance. It’s bare-minimum in the sense that only the Parameters and Resources which are absolutely necessary are part of the template.&lt;/p&gt;

&lt;p&gt;This template can also execute a bash script provided as &lt;a href=&quot;https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/user-data.html#user-data-shell-scripts&quot;&gt;User Data&lt;/a&gt; to the EC2 instance. The User Data can be a set of directives that will be executed only once during the first boot cycle of the instance. This feature can be used when you need to say launch several instances with the Apache web server installed on them. To do this you would simply supply the command &lt;code class=&quot;highlighter-rouge&quot;&gt;apt-get install apache2&lt;/code&gt; as user-data. There is no need to add sudo keyword to the command as the script will be run with root privileges.&lt;/p&gt;

&lt;p&gt;In the above template, the custom script updates the list of packages (apt-get update) and installs Apache (apt-get install apache2). You can add/remove commands as required. Be sure to put them inside double quotes and each command should be on a new line.&lt;/p&gt;</content><author><name>Akash</name></author><category term="automation" /><category term="cloudformation" /><category term="devops" /><category term="ec2" /><category term="shell" /><summary type="html">The following is a bare-minimum CloudFormation template to launch an EC2 instance. It’s bare-minimum in the sense that only the Parameters and Resources which are absolutely necessary are part of the template. This template can also execute a bash script provided as User Data to the EC2 instance. The User Data can be a set of directives that will be executed only once during the first boot cycle of the instance. This feature can be used when you need to say launch several instances with the Apache web server installed on them. To do this you would simply supply the command apt-get install apache2 as user-data. There is no need to add sudo keyword to the command as the script will be run with root privileges. In the above template, the custom script updates the list of packages (apt-get update) and installs Apache (apt-get install apache2). You can add/remove commands as required. Be sure to put them inside double quotes and each command should be on a new line.</summary></entry></feed>