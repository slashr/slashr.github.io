<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator><link href="https://akashnair.com/feed.xml" rel="self" type="application/atom+xml" /><link href="https://akashnair.com/" rel="alternate" type="text/html" /><updated>2021-12-18T09:45:47+01:00</updated><id>https://akashnair.com/feed.xml</id><title type="html">Akash Nair</title><subtitle>I am a DevOps Engineer and infrastructure nerd. I design and build on the cloud.</subtitle><author><name>Akash Nair</name></author><entry><title type="html">Observable Kubernetes Cluster Using Grafana-Loki-Prometheus</title><link href="https://akashnair.com/grafana-loki-prometheus/" rel="alternate" type="text/html" title="Observable Kubernetes Cluster Using Grafana-Loki-Prometheus" /><published>2021-11-04T00:00:00+01:00</published><updated>2021-11-04T00:00:00+01:00</updated><id>https://akashnair.com/infrastructure-engineering-using-terraform-and-gitlab</id><content type="html" xml:base="https://akashnair.com/grafana-loki-prometheus/">### Introduction ###

With the growing popularity of Kubernetes and the number of options to easily deploy a K8S cluster quickly, organisations are becoming more confident of hosting a SaaS in-house rather than use it as a subscription. 
However, when you ship a Terraform moulded K8S cluster, it is important that you also get visibility into the cluster as well as the application. 

The best way of achieving this is to use Terraform and Helm to install basic apps on the cluster such as the Grafana-Loki-Promtheus stack


&lt;center&gt;&lt;img src=&quot;../assets/images/eyeofgrafana.png&quot; height=&quot;400&quot; width=&quot;400&quot;&gt;&lt;/center&gt;
&lt;!--more--&gt;

---

### Step 1: 
Use the Terraform Module to install the Loki Helm Chart (with Grafana and Prometheus disabled, and Promtail enabled). This is because when installing Grafana using Loki, it installs a rather outdated version of Grafana. 
Thus it is better to use the Grafana Helm Chart to install the latest version of Grafana and Prometheus. 

---

### Step 2: 
Configuration of Loki inside Grafana

```
enabled: true
check_for_updates: false

plugins:
  - grafana-piechart-panel
  - vonage-status-panel

datasources: 
  datasources.yaml:
    apiVersion: 1
    datasources:
    - name: Loki
      type: loki
      access: proxy
      url: http://loki-stack:3100
      jsonData:
        maxLines: 1000
        derivedFields:
         # Field with external link.
          - matcherRegex: &quot;traceID=(\\w+)&quot;
            name: TraceID
            url: &apos;http://loki-stack:3100&apos;
```

As you can see, there is also the possiblity of parsing logs using the Derived Fields feature of Loki. This rule would however apply to all logs and it is therefore preferred to do all parsing using Promtail. 

You can add the Loki datasource inside the values.yaml of Grafana so that it is pre-configured. The URL should match the Service name of the Loki stack

---

### Step 3: 

Optionally, configure Promtail to blacklist unecessary namespaces such as kube-system, monitoring etc. This can be done by adding the following code in every `relabel_configs` part of every job in the Promtail configuration. The best way to do this is to copy the default ConfigMap of Promtail and paste it in the values.yaml file of the Loki Stack under promatil.scrapeConfigs

However, this can become a problem when in the future updates to the ConfigMap template inside the Helm Charts are made and it won&apos;t be reflected on your ConfigMap until it is updated manually. So until, we have the ability to drop logs based on namespace, pod-name etc using the Values file, I would advice against doing this. 

      relabel_configs:
      - action: drop
        regex: &apos;kube-system&apos;
        source_labels:
        - __meta_kubernetes_namespace

---

### Step 4: 

Next, in order to Parse the logs to extract the most useful information, add a new Job under `promatil.extraScrapeConfigs` in the Values file with the following code:

```
    - job_name: custom_logs_scraping
      kubernetes_sd_configs:
      - role: pod
      pipeline_stages:
      - docker: {}
        # Regex expressions for extraction of specific metrics/keywords from the logs
        - regex:
            expression: &apos;\&quot;traceId\&quot;..\&quot;(?P&lt;traceId&gt;.+?)\&quot;&apos;
        - regex:
            expression: \&quot;ElapsedMilliseconds\&quot;..(?P&lt;responseTime&gt;[\d\.]+)
        - regex:
            expression: &apos;\&quot;StatusCode\&quot;..(?P&lt;statusCode&gt;.\d+)&apos;
        - regex: 
            expression: ^.*(?P&lt;finishedRequests&gt;Request finished).*$
        # Converts extracted values above into log labels
        - labels:
            traceId:
            responseTime:
            statusCode:
            finishedRequests:
        # Creates metrics from the extracted label
        # The metrics can be viewed in Grafana with Prometheus as data source
        - metrics:
            total_finished_requests_counter:
              type: Counter
              description: &quot;Total number of requests finished successfully&quot;
              prefix: myapp_log_metrics_
              source: finishedRequests
              config:
                action: inc
        - metrics:
            http_response_time_milliseconds:
              type: Histogram
              description: &quot;HTTP Request response times&quot;
              prefix: myapp_log_metrics_
              source: responseTime
              config:
                buckets: [0.25,0.5,1.0,1.25,1.5,1.75]
      relabel_configs:
      - action: drop
        regex: &apos;custom-metrics&apos;
        source_labels:
        - __meta_kubernetes_namespace
      - action: drop
        regex: .+
        source_labels:
        - __meta_kubernetes_pod_label_name
      - source_labels:
        - __meta_kubernetes_pod_label_app
        - __meta_kubernetes_pod_node_name
        target_label: __host__
      - action: drop
        regex: &apos;&apos;
        source_labels:
        - __service__
      - action: labelmap
        regex: __meta_kubernetes_pod_label_(.+)
      - action: replace
        replacement: $1
        separator: /
        source_labels:
        - __meta_kubernetes_namespace
        - __service__
        target_label: job
      - action: replace
        source_labels:
        - __meta_kubernetes_namespace
        target_label: namespace
      - action: replace
        source_labels:
```

Using the Regex above and given the log entry below: 

- `2021-07-15T09:28:20.888238937Z stdout F { &quot;timestamp&quot;: &quot;2021-07-15T09:28:20.881Z&quot;, &quot;severity&quot;: &quot;Info&quot;, &quot;message&quot;: Request finished in 0.2358ms 200 application/grpc, &quot;traceId&quot;: &quot;969814f6-40ac9c957e3abdf8&quot;, &quot;additionalData&quot;: { &quot;ElapsedMilliseconds&quot;: 0.2358, &quot;StatusCode&quot;: 200, &quot;ContentType&quot;: &quot;application\/grpc&quot;} }`

Promtail would extract the following four labels:

- traceId: 969814f6-40ac9c957e3abdf8
- responseTime: 0.2358
- statusCode: 200
- finishedRequests: 1

The above snippet of code also has the stage `metrics`. Metrics will use the extracted value specified by `source` and perform an action on it specified by `config.action`. There are three types of metrics available using Promtail currently: Counter, Gauge and Histogram. These metrics will then be read and exposed by Prometheus. 

---

### Step 5: 
You can create custom Dashboards using the above metrics extracted from your logs in Grafan. Create a new Dashboard, then add a Panel, select Prometheus as the Datasource and then search for the name of the metric exposed (note that it will begin with the prefix &quot;myapp_log_metrics&quot;) as specified in the snippet above. 

After creating this Dashoboard, you can export them to a JSON file. Add these JSON files to your repository, and create a ConfigMap with `metadata.labels.grafana_dashboard: &quot;true&quot;` and add the JSON files under `data`. 

This will make Grafana automatically import the dashboards on lauch. 

Example of this configuration can be found [here](https://github.com/pivotal-cf/charts-grafana/blob/master/templates/dashboard-configMap.yaml)

Finally, this setup can be packaged into a Helm chart and published in a chart repo. And thus, you can finally add the chart to be deployed using a Terraform Helm module and thus make this monitoring stack plug-and-playable.</content><author><name>Akash</name></author><category term="Automation" /><category term="helm" /><category term="kubernetes" /><category term="docker" /><summary type="html">### Introduction ### With the growing popularity of Kubernetes and the number of options to easily deploy a K8S cluster quickly, organisations are becoming more confident of hosting a SaaS in-house rather than use it as a subscription. However, when you ship a Terraform moulded K8S cluster, it is important that you also get visibility into the cluster as well as the application. The best way of achieving this is to use Terraform and Helm to install basic apps on the cluster such as the Grafana-Loki-Promtheus stack</summary></entry><entry><title type="html">Observable Kubernetes Cluster Using Grafana-Loki-Prometheus</title><link href="https://akashnair.com/grafana-loki-prometheus/" rel="alternate" type="text/html" title="Observable Kubernetes Cluster Using Grafana-Loki-Prometheus" /><published>2021-07-16T00:00:00+02:00</published><updated>2021-07-16T00:00:00+02:00</updated><id>https://akashnair.com/observable-k8s-cluster-using-grafana-loki-prometheus</id><content type="html" xml:base="https://akashnair.com/grafana-loki-prometheus/">&lt;h3 id=&quot;introduction&quot;&gt;Introduction&lt;/h3&gt;

&lt;p&gt;With the growing popularity of Kubernetes and the number of options to easily deploy a K8S cluster quickly, organisations are becoming more confident of hosting a SaaS in-house rather than use it as a subscription. 
However, when you ship a Terraform moulded K8S cluster, it is important that you also get visibility into the cluster as well as the application.&lt;/p&gt;

&lt;p&gt;The best way of achieving this is to use Terraform and Helm to install basic apps on the cluster such as the Grafana-Loki-Promtheus stack&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;../assets/images/eyeofgrafana.png&quot; height=&quot;400&quot; width=&quot;400&quot; /&gt;&lt;/center&gt;
&lt;!--more--&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;step-1&quot;&gt;Step 1:&lt;/h3&gt;
&lt;p&gt;Use the Terraform Module to install the Loki Helm Chart (with Grafana and Prometheus disabled, and Promtail enabled). This is because when installing Grafana using Loki, it installs a rather outdated version of Grafana. 
Thus it is better to use the Grafana Helm Chart to install the latest version of Grafana and Prometheus.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;step-2&quot;&gt;Step 2:&lt;/h3&gt;
&lt;p&gt;Configuration of Loki inside Grafana&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;enabled: true
check_for_updates: false

plugins:
  - grafana-piechart-panel
  - vonage-status-panel

datasources: 
  datasources.yaml:
    apiVersion: 1
    datasources:
    - name: Loki
      type: loki
      access: proxy
      url: http://loki-stack:3100
      jsonData:
        maxLines: 1000
        derivedFields:
         # Field with external link.
          - matcherRegex: &quot;traceID=(\\w+)&quot;
            name: TraceID
            url: &apos;http://loki-stack:3100&apos;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;As you can see, there is also the possiblity of parsing logs using the Derived Fields feature of Loki. This rule would however apply to all logs and it is therefore preferred to do all parsing using Promtail.&lt;/p&gt;

&lt;p&gt;You can add the Loki datasource inside the values.yaml of Grafana so that it is pre-configured. The URL should match the Service name of the Loki stack&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;step-3&quot;&gt;Step 3:&lt;/h3&gt;

&lt;p&gt;Optionally, configure Promtail to blacklist unecessary namespaces such as kube-system, monitoring etc. This can be done by adding the following code in every &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;relabel_configs&lt;/code&gt; part of every job in the Promtail configuration. The best way to do this is to copy the default ConfigMap of Promtail and paste it in the values.yaml file of the Loki Stack under promatil.scrapeConfigs&lt;/p&gt;

&lt;p&gt;However, this can become a problem when in the future updates to the ConfigMap template inside the Helm Charts are made and it won’t be reflected on your ConfigMap until it is updated manually. So until, we have the ability to drop logs based on namespace, pod-name etc using the Values file, I would advice against doing this.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  relabel_configs:
  - action: drop
    regex: &apos;kube-system&apos;
    source_labels:
    - __meta_kubernetes_namespace
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;step-4&quot;&gt;Step 4:&lt;/h3&gt;

&lt;p&gt;Next, in order to Parse the logs to extract the most useful information, add a new Job under &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;promatil.extraScrapeConfigs&lt;/code&gt; in the Values file with the following code:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    - job_name: custom_logs_scraping
      kubernetes_sd_configs:
      - role: pod
      pipeline_stages:
      - docker: {}
        # Regex expressions for extraction of specific metrics/keywords from the logs
        - regex:
            expression: &apos;\&quot;traceId\&quot;..\&quot;(?P&amp;lt;traceId&amp;gt;.+?)\&quot;&apos;
        - regex:
            expression: \&quot;ElapsedMilliseconds\&quot;..(?P&amp;lt;responseTime&amp;gt;[\d\.]+)
        - regex:
            expression: &apos;\&quot;StatusCode\&quot;..(?P&amp;lt;statusCode&amp;gt;.\d+)&apos;
        - regex: 
            expression: ^.*(?P&amp;lt;finishedRequests&amp;gt;Request finished).*$
        # Converts extracted values above into log labels
        - labels:
            traceId:
            responseTime:
            statusCode:
            finishedRequests:
        # Creates metrics from the extracted label
        # The metrics can be viewed in Grafana with Prometheus as data source
        - metrics:
            total_finished_requests_counter:
              type: Counter
              description: &quot;Total number of requests finished successfully&quot;
              prefix: myapp_log_metrics_
              source: finishedRequests
              config:
                action: inc
        - metrics:
            http_response_time_milliseconds:
              type: Histogram
              description: &quot;HTTP Request response times&quot;
              prefix: myapp_log_metrics_
              source: responseTime
              config:
                buckets: [0.25,0.5,1.0,1.25,1.5,1.75]
      relabel_configs:
      - action: drop
        regex: &apos;custom-metrics&apos;
        source_labels:
        - __meta_kubernetes_namespace
      - action: drop
        regex: .+
        source_labels:
        - __meta_kubernetes_pod_label_name
      - source_labels:
        - __meta_kubernetes_pod_label_app
        - __meta_kubernetes_pod_node_name
        target_label: __host__
      - action: drop
        regex: &apos;&apos;
        source_labels:
        - __service__
      - action: labelmap
        regex: __meta_kubernetes_pod_label_(.+)
      - action: replace
        replacement: $1
        separator: /
        source_labels:
        - __meta_kubernetes_namespace
        - __service__
        target_label: job
      - action: replace
        source_labels:
        - __meta_kubernetes_namespace
        target_label: namespace
      - action: replace
        source_labels:
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Using the Regex above and given the log entry below:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;2021-07-15T09:28:20.888238937Z stdout F { &quot;timestamp&quot;: &quot;2021-07-15T09:28:20.881Z&quot;, &quot;severity&quot;: &quot;Info&quot;, &quot;message&quot;: Request finished in 0.2358ms 200 application/grpc, &quot;traceId&quot;: &quot;969814f6-40ac9c957e3abdf8&quot;, &quot;additionalData&quot;: { &quot;ElapsedMilliseconds&quot;: 0.2358, &quot;StatusCode&quot;: 200, &quot;ContentType&quot;: &quot;application\/grpc&quot;} }&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Promtail would extract the following four labels:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;traceId: 969814f6-40ac9c957e3abdf8&lt;/li&gt;
  &lt;li&gt;responseTime: 0.2358&lt;/li&gt;
  &lt;li&gt;statusCode: 200&lt;/li&gt;
  &lt;li&gt;finishedRequests: 1&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The above snippet of code also has the stage &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;metrics&lt;/code&gt;. Metrics will use the extracted value specified by &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;source&lt;/code&gt; and perform an action on it specified by &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;config.action&lt;/code&gt;. There are three types of metrics available using Promtail currently: Counter, Gauge and Histogram. These metrics will then be read and exposed by Prometheus.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;step-5&quot;&gt;Step 5:&lt;/h3&gt;
&lt;p&gt;You can create custom Dashboards using the above metrics extracted from your logs in Grafan. Create a new Dashboard, then add a Panel, select Prometheus as the Datasource and then search for the name of the metric exposed (note that it will begin with the prefix “myapp_log_metrics”) as specified in the snippet above.&lt;/p&gt;

&lt;p&gt;After creating this Dashoboard, you can export them to a JSON file. Add these JSON files to your repository, and create a ConfigMap with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;metadata.labels.grafana_dashboard: &quot;true&quot;&lt;/code&gt; and add the JSON files under &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;data&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;This will make Grafana automatically import the dashboards on lauch.&lt;/p&gt;

&lt;p&gt;Example of this configuration can be found &lt;a href=&quot;https://github.com/pivotal-cf/charts-grafana/blob/master/templates/dashboard-configMap.yaml&quot;&gt;here&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Finally, this setup can be packaged into a Helm chart and published in a chart repo. And thus, you can finally add the chart to be deployed using a Terraform Helm module and thus make this monitoring stack plug-and-playable.&lt;/p&gt;</content><author><name>Akash</name></author><category term="Automation" /><category term="helm" /><category term="kubernetes" /><category term="docker" /><summary type="html">Introduction With the growing popularity of Kubernetes and the number of options to easily deploy a K8S cluster quickly, organisations are becoming more confident of hosting a SaaS in-house rather than use it as a subscription. However, when you ship a Terraform moulded K8S cluster, it is important that you also get visibility into the cluster as well as the application. The best way of achieving this is to use Terraform and Helm to install basic apps on the cluster such as the Grafana-Loki-Promtheus stack</summary></entry><entry><title type="html">Migrate from Helm 2 to Helm 3 Safely</title><link href="https://akashnair.com/migrate-helm2-to-helm3/" rel="alternate" type="text/html" title="Migrate from Helm 2 to Helm 3 Safely" /><published>2021-01-01T00:00:00+01:00</published><updated>2021-01-01T00:00:00+01:00</updated><id>https://akashnair.com/migrate-from-helm2-to-helm3-safely</id><content type="html" xml:base="https://akashnair.com/migrate-helm2-to-helm3/">&lt;h3 id=&quot;introduction&quot;&gt;Introduction&lt;/h3&gt;

&lt;p&gt;As you have probably already read, Helm 2 has been replaced by Helm 3 which comes with a lot of major changes, the biggest being that the Tiller pod has been removed. This is a great security improvement since Tiller was a pod running with a lot of privileges on the cluster and was major point of vulnerability.&lt;/p&gt;

&lt;p&gt;Also, support and upgrades to Helm 2 stopped since November 13, 2020. More information can be found &lt;a href=&quot;https://helm.sh/blog/helm-v2-deprecation-timeline/&quot;&gt;here&lt;/a&gt;. So now is a good time, to finally make that move to Helm 3.&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;../assets/images/helm.png&quot; height=&quot;400&quot; width=&quot;400&quot; /&gt;&lt;/center&gt;
&lt;!--more--&gt;

&lt;h3 id=&quot;setup--overview&quot;&gt;Setup &amp;amp; Overview&lt;/h3&gt;

&lt;p&gt;The basic flow of the migration will be as follows&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Backup data&lt;/li&gt;
  &lt;li&gt;Let Helm 2 continue to handled the Releases&lt;/li&gt;
  &lt;li&gt;Install Helm 3 client and configure it to be able to access the Helm 2 Releases&lt;/li&gt;
  &lt;li&gt;Run a Helm 3 upgrade dry run on the Releases and fix any conflicts&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Finally run a Helm 3 upgrade on the Releases and if successful, delete Helm 2 &amp;amp; Tiller configuration from the cluster&lt;/p&gt;

    &lt;p&gt;&lt;strong&gt;Install the v2 and v3 clients on your machine&lt;/strong&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;ul&gt;
  &lt;li&gt;Download and save the Helm 3 binary from &lt;a href=&quot;https://github.com/helm/helm/releases&quot;&gt;here&lt;/a&gt; and save it in your binary path as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;helm3&lt;/code&gt;. For example, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mv helm-v3.4.2-darwin-amd64/helm /usr/local/bin/helm3&lt;/code&gt; It’s important to name this as helm3 so that you don’t confuse the two versions while performing the migration.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Similarly, download and save the most recent Helm 2 binary from the releases page and save it as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;helm2&lt;/code&gt; in your binary path. Then run &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;helm2 list&lt;/code&gt; to see if the version is compatible with your Tiller version. If not, then download a older helm2 version accordingly.&lt;/p&gt;

    &lt;p&gt;&lt;strong&gt;Backup&lt;/strong&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Install the backup plugin using helm2 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;helm plugin install https://github.com/maorfr/helm-backup&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Next, backup all the namespaces where Helm Releases exist using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;helm backup &amp;lt;namespace&amp;gt;&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;migration&quot;&gt;Migration&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;Install the migration plugin using helm3 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;https://github.com/helm/helm-2to3&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;There are two major steps to the migration.
    &lt;ul&gt;
      &lt;li&gt;Move Config: This copies the configuration of the Releases into the Helm3 config directory.&lt;/li&gt;
      &lt;li&gt;Convert: This will create links to the Helm2 Releases in the Helm3 config directory so that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;helm3&lt;/code&gt; can access them. After this &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;helm3 list&lt;/code&gt; should show the currently deployed Releases&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;So first, run &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;helm3 2to3 move config --dry-run&lt;/code&gt; to make sure there are no errors and then subsequently run it without the –dry-run flag.&lt;/li&gt;
  &lt;li&gt;Next, run &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;helm3 2to3 convert &amp;lt;release-name&amp;gt; --dry-run&lt;/code&gt; to ensure a error-free run and then execute the command without the –dry-run flag&lt;/li&gt;
  &lt;li&gt;You should now be able to list the Releases using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;helm3 list&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;As a safety check, do a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kubectl get pods,svc&lt;/code&gt; on the resources running under the Helm2 Releases. Check the Age of these resources to make sure that they weren’t redeployed.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;It’s important to note that this migration tool only migrates the Helm2 configuration, not the Kubernetes Resources themselves&lt;/p&gt;

&lt;h3 id=&quot;upgrade-conflicts&quot;&gt;Upgrade Conflicts&lt;/h3&gt;
&lt;p&gt;Now that you have successfully “copied” the Releases from Helm2 to Helm3, it’s time to test how well the migration worked. In my personal experience, some of the Releases work out of the box with Helm3, i.e, running &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;helm3 upgrade &amp;lt;release&amp;gt;&lt;/code&gt; works as expected and upgrades the Release without anything breaking.&lt;/p&gt;

&lt;p&gt;However, for some Releases, especially the older ones and outdated ones which have gone through several version upgrades that have Breaking Changes, I saw peculiar issues like these.&lt;/p&gt;

&lt;p&gt;Some upgrades failed because of outdated annotations added to the K8S objects by the Helm2 client. To fix this, run &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;helm3 upgrade grafana stable/grafana --dry-run&lt;/code&gt; and then manually annotate the Objects that are shown as incorrectly annotated as follows in the case of a Grafana chart&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kubectl annotate Deployment grafana &quot;meta.helm.sh/release-name=grafana&quot; &quot;meta.helm.sh/release-namespace=default&quot; --overwrite&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kubectl label ServiceAccount grafana-test &quot;app.kubernetes.io/managed-by=Helm&quot;&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Similary, run &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;helm3 upgrade grafana stable/grafana --dry-run&lt;/code&gt; again until the errors disappear and finally run the upgrade without the –dry-run flag&lt;/p&gt;

&lt;h3 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;So while migrating the Releases to Helm3 from Helm2 is pretty much non-breaking and safe to perform, the real challenge is making these migrated Releases compatible with Helm3. Helm3 also uses custom chart repository instead of the old stable/charts central repository. This means that you will have to look up the latest repo URL for your chart and add them Helm3 using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;helm repo add &amp;lt;repo-name&amp;gt; &amp;lt;repo-url&amp;gt;&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;As a best practice, please make sure to go through the Breaking Changes if any of whatever app you are trying to upgrade. Also, perform minor point version upgrades and slowly get to the version you are comfortable with or is stable.&lt;/p&gt;

&lt;p&gt;Things get more complicated when upgrading Releases which have Persistent Storage or stuff like MongoDB Deployments. These should be handled much more carefully, making sure to take backups and doing a dry run on a identical setup on a non-production cluster.&lt;/p&gt;

&lt;p&gt;Some Deployments like Nginx-Ingress might be too outdated to be able to maintain them using Helm3. In such cases, it makes sense to create a new Helm Release from scratch rather than spend a lot of hours trying to do a in-place upgrade.&lt;/p&gt;</content><author><name>Akash</name></author><category term="Automation" /><category term="helm" /><category term="kubernetes" /><category term="docker" /><summary type="html">Introduction As you have probably already read, Helm 2 has been replaced by Helm 3 which comes with a lot of major changes, the biggest being that the Tiller pod has been removed. This is a great security improvement since Tiller was a pod running with a lot of privileges on the cluster and was major point of vulnerability. Also, support and upgrades to Helm 2 stopped since November 13, 2020. More information can be found here. So now is a good time, to finally make that move to Helm 3.</summary></entry><entry><title type="html">Jenkins, Kaniko, Kubernetes: The Cloud CI/CD Trifecta</title><link href="https://akashnair.com/jenkins-kaniko-kubernetes-cloud-ci-cd/" rel="alternate" type="text/html" title="Jenkins, Kaniko, Kubernetes: The Cloud CI/CD Trifecta" /><published>2019-06-24T00:00:00+02:00</published><updated>2019-06-24T00:00:00+02:00</updated><id>https://akashnair.com/jenkins-kaniko-kubernetes-the-cloud-ci-cd-trifecta</id><content type="html" xml:base="https://akashnair.com/jenkins-kaniko-kubernetes-cloud-ci-cd/">&lt;h3 id=&quot;introduction&quot;&gt;Introduction&lt;/h3&gt;
&lt;p&gt;Running Jenkins on Kubernetes unleashes the scalability powers of Jenkins and makes it easier to replicate and set it up. It involves running a Jenkins Master server (&lt;a href=&quot;https://hub.docker.com/r/jenkinsci/blueocean&quot;&gt;jenkinsci/blueocean&lt;/a&gt;) as a StatefulSet and connecting it with the Kubernetes cluster so that the master can spawn “jenkins slaves” on the K8S cluster whenever a build job is triggered. These slaves are nothing but K8S pods consisting of containers based on the &lt;a href=&quot;https://hub.docker.com/r/jenkinsci/jnlp-slave&quot;&gt;jenkinsci/jnlp-slave&lt;/a&gt; docker image from Docker Hub.&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;https://jenkins.io/images/jenkins-x/logo.svg&quot; height=&quot;400&quot; width=&quot;400&quot; /&gt;&lt;/center&gt;
&lt;!--more--&gt;

&lt;h3 id=&quot;step-i-create-and-deploy-jenkins-master&quot;&gt;Step I: Create and deploy Jenkins master&lt;/h3&gt;

&lt;p&gt;We first need to create a &lt;strong&gt;Dockerfile&lt;/strong&gt; for Jenkins master based on &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;jenkinsci/blueocean&lt;/code&gt; and install the &lt;strong&gt;Kuberentes plug-in&lt;/strong&gt; inside it along with any other optional plug-ins. Build this image and push it to a docker registry&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;Refer: &lt;a href=&quot;https://github.com/slashr/jenkins-on-kubernetes/blob/master/Dockerfile.jenkins.master&quot;&gt;Dockerfile.jenkins.master&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Next deploy the Jenkins master image on a pod on Kubernetes. Note that although you can create a normal kuberentes &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Deployment&lt;/code&gt; for this, it is recommended that you create a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;StatefulSet&lt;/code&gt; instead. This ensures the state and configuration of the pod is maintained. In addition to it being a StatefulSet a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PersistentVolume&lt;/code&gt; and a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PersistentVolumeClaim&lt;/code&gt; is required to maintain the data stored inside the Jenkins master.&lt;/p&gt;

&lt;p&gt;With the above setup, even if we terminate the pods running Jenkins master, the state and data will persist and the new pod will launch with the existing settings in place.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Refer to this manifest to understand how to setup a PV and PVC: &lt;a href=&quot;https://github.com/slashr/jenkins-on-kubernetes/blob/master/persistent-volume.yaml&quot;&gt;persistent-volume.yaml&lt;/a&gt; Make sure to replace &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;spec.awsElasticBlockStore.volumeID&lt;/code&gt; with your own EBS volume ID.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;Jenkins master deployment manifest: &lt;a href=&quot;https://github.com/slashr/jenkins-on-kubernetes/blob/master/jenkins-deploy.yml&quot;&gt;jenkins-deploy.yml&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;step-ii-connect-jenkins-with-the-kubernetes-cluster&quot;&gt;Step II: Connect Jenkins with the Kubernetes cluster&lt;/h3&gt;
&lt;p&gt;We will first create a service for the master Jenkins server and the slaves. Service type for master should be &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;LoadBalancer&lt;/code&gt; and for slaves should be &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ClusterIP&lt;/code&gt;. The ports required on the master are 80 (optionally 443) and on the slaves port 50000 (default JNLP port for communication between master and the slave). Refer the following file: &lt;a href=&quot;https://github.com/slashr/jenkins-on-kubernetes/blob/master/jenkins-service.yml&quot;&gt;jenkins-service.yml&lt;/a&gt;
  Optionally, you can bind the Jenkins master ELB URL with a custom domain (jenkins.example.com) by create a A Record (Alias type) in Route 53.&lt;/p&gt;

&lt;p&gt;To provide Jenkins access to the Kubernetes cluster so that it can create pods for every build job, perform the following steps&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Click on &lt;strong&gt;Manage Jenkins&lt;/strong&gt; on the Jenkins homepage and then click on &lt;strong&gt;Configure System&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;Disable the workers on the master Jenkins pod &lt;strong&gt;# of executors&lt;/strong&gt; to 0. This ensures that build jobs are run solely on slaves and not on the master&lt;/li&gt;
  &lt;li&gt;Scroll down to the end of the page and click on &lt;strong&gt;Add a new cloud&lt;/strong&gt; and then click on Kubernetes.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Enter&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Name&lt;/strong&gt; for the cluster&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Kubernetes URL&lt;/strong&gt; E.g https://api.kube-cluster.example.com. You can find your API endpoint by running &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kubectl cluster-info&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Kubernetes Namespace&lt;/strong&gt;: It’s the namespace where you deployed your Jenkins master pod&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Credentials&lt;/strong&gt;: Enter the username/password of your cluster (located in ~/.kube/config). Click on &lt;strong&gt;Test Connection&lt;/strong&gt; to see if the credentials are correct and Jenkins is able to connect to Kubernetes.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Jenkins URL&lt;/strong&gt;: Enter the endpoint of your Jenkins Master service (could be an ELB URL or a custom domain)&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Jenkins Tunnel&lt;/strong&gt;: Enter the endpoint of your Jenkins Slave service created earlier. It’s usually a cluster IP along with a port (usually 5000). It should look like 100.76.43.23:50000. The Jenkins master connects to the Kubernetes cluster using this endpoint.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Leave everything else as it is and then click on &lt;strong&gt;Save&lt;/strong&gt;.&lt;/p&gt;

&lt;h3 id=&quot;step-iii-write-the-jenkinsfile&quot;&gt;Step III: Write the Jenkinsfile&lt;/h3&gt;

&lt;p&gt;Next, we need to define the template for the Kubernetes pods that will build our application. This is best done from inside the Jenkinsfile. You can find the Jenkinsfile template here: &lt;a href=&quot;https://github.com/slashr/jenkins-on-kubernetes/blob/master/Jenkinsfile&quot;&gt;Jenkinsfile&lt;/a&gt; 
  As you can see, we have three containers running in the pod.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Kaniko&lt;/strong&gt;: For building our docker images and pushing them to our ECR registry&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;kubectl&lt;/strong&gt;: To deploy our application on Kubernetes&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;jnlp-slave&lt;/strong&gt;: The jenkins slave container although not defined in the Jenkinsfile is automatically created inside the pod by Jenkins master.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We have three volumes:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;kube-config&lt;/strong&gt;: It’s the ~/.kube/config file of your Kubernetes cluster and is mounted on the kubectl container for authentication to the K8S cluster&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;aws-secret&lt;/strong&gt;: It’s the ~/.aws/credentials file required by Kaniko to authenticate with ECR and publish the docker images&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;docker-config&lt;/strong&gt;: It’s a simple JSON file consisting of the ECR registry endpoint. Example: &lt;a href=&quot;https://github.com/slashr/jenkins-on-kubernetes/blob/master/docker-registry-config.yaml&quot;&gt;https://github.com/slashr/jenkins-on-kubernetes/blob/master/docker-registry-config.yaml&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;podlabel&lt;/strong&gt;: this is a variable we use to uniquely name the pods that will be spawned. If we don’t use it, then there are problems creating pods parallel for each job in the queue.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Note that for registries other an ECR such as GCR, you would need an additional “authentication token” inside this file.&lt;/p&gt;

&lt;h3 id=&quot;step-iv-create-a-pipeline&quot;&gt;Step IV: Create a pipeline&lt;/h3&gt;
&lt;p&gt;Now that Jenkins and Kubernetes setup is complete, we can create a Multibranch Pipeline and see the pods being spawned for builds in action! Simply click on &lt;strong&gt;New Item&lt;/strong&gt; on the Jenkins Homepage, enter a name for the project and click on &lt;strong&gt;Multibranch Pipeline&lt;/strong&gt; and click OK.&lt;/p&gt;

&lt;p&gt;Then click on &lt;strong&gt;Add Source&lt;/strong&gt; and your git repository. Click on Save and then Jenkins will start scanning your repository and adding all branches which contain a Jenkinsfile in the root directory.&lt;/p&gt;

&lt;p&gt;That’s it! You will see a few pods being spawned already for the new branches in the “jenkins” namespace. From now onwards, pods will be spawned on Kubernetes for every build and terminated after the build is finished&lt;/p&gt;</content><author><name>Akash</name></author><category term="Automation" /><category term="jenkins" /><category term="kubernetes" /><category term="kaniko" /><category term="jenkinsfile" /><category term="dind" /><summary type="html">Introduction Running Jenkins on Kubernetes unleashes the scalability powers of Jenkins and makes it easier to replicate and set it up. It involves running a Jenkins Master server (jenkinsci/blueocean) as a StatefulSet and connecting it with the Kubernetes cluster so that the master can spawn “jenkins slaves” on the K8S cluster whenever a build job is triggered. These slaves are nothing but K8S pods consisting of containers based on the jenkinsci/jnlp-slave docker image from Docker Hub.</summary></entry><entry><title type="html">Suppressing Fires using Gas: An IoT Experiment</title><link href="https://akashnair.com/fire-suppression-using-smart-sensors-iot/" rel="alternate" type="text/html" title="Suppressing Fires using Gas: An IoT Experiment" /><published>2018-03-17T22:03:52+01:00</published><updated>2018-03-17T22:03:52+01:00</updated><id>https://akashnair.com/fire-suppression-using-smart-sensors-iot</id><content type="html" xml:base="https://akashnair.com/fire-suppression-using-smart-sensors-iot/">&lt;p&gt;An IoT experiment to create a gas based suppression system that can be used to extinguish fires inside water sensitive enclosures such as data centers. We make use of Arduino microcontrollers and several sensors to create this system.&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;/assets/images/arduino-logo.svg&quot; height=&quot;400&quot; width=&quot;400&quot; /&gt;&lt;/center&gt;
&lt;!--more--&gt;
&lt;hr /&gt;

&lt;h2 id=&quot;the-design&quot;&gt;The Design&lt;/h2&gt;

&lt;p&gt;A fully autonomous monitoring and suppression system that detects fire and extinguishes it using inert gases while also ensuring safety and avoiding danger at the same time. This system consists of the following functionalities:&lt;/p&gt;

&lt;ul style=&quot;list-style-type: square;&quot;&gt;
  &lt;li&gt;
    The detection of fire using real-time data from fire and temperature sensors
  &lt;/li&gt;
  &lt;li&gt;
    The detection of personnel inside the data center using feedback from presence&lt;br /&gt; sensors
  &lt;/li&gt;
  &lt;li&gt;
    The release of gas to suppress the fire and continuous monitoring based on real-time&lt;br /&gt; feedback from fire and temperature sensors
  &lt;/li&gt;
  &lt;li&gt;
    The detection of any leftover gas or fire using feedback from the fire, temperature&lt;br /&gt; and gas sensors
  &lt;/li&gt;
  &lt;li&gt;
    The interfacing of the system using an HMI to allow monitoring and manual intervention in case of a unexpected event of malfunction.
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;the-architecture&quot;&gt;The Architecture&lt;/h2&gt;

&lt;p&gt;The system consist of the following sensors and devices&lt;/p&gt;

&lt;table style=&quot;height: 171px; width: 488px;&quot;&gt;
  &lt;tr&gt;
    &lt;td style=&quot;width: 164px;&quot;&gt;
      Device
    &lt;/td&gt;
    
    &lt;td style=&quot;width: 310px;&quot;&gt;
      Purpose
    &lt;/td&gt;
  &lt;/tr&gt;
  
  &lt;tr&gt;
    &lt;td style=&quot;width: 164px;&quot;&gt;
      MQ2 Gas sensor
    &lt;/td&gt;
    
    &lt;td style=&quot;width: 310px;&quot;&gt;
      Detection of inert gas
    &lt;/td&gt;
  &lt;/tr&gt;
  
  &lt;tr&gt;
    &lt;td style=&quot;width: 164px;&quot;&gt;
      MQ2 temperature sensor
    &lt;/td&gt;
    
    &lt;td style=&quot;width: 310px;&quot;&gt;
      Detection of fire (measures temperature)
    &lt;/td&gt;
  &lt;/tr&gt;
  
  &lt;tr&gt;
    &lt;td style=&quot;width: 164px;&quot;&gt;
      Magideal IR Infrared Fire sensor
    &lt;/td&gt;
    
    &lt;td style=&quot;width: 310px;&quot;&gt;
      Detection of fire (identifies fire)
    &lt;/td&gt;
  &lt;/tr&gt;
  
  &lt;tr&gt;
    &lt;td style=&quot;width: 164px;&quot;&gt;
      L9110 fans
    &lt;/td&gt;
    
    &lt;td style=&quot;width: 310px;&quot;&gt;
      Exhaust and release of gas
    &lt;/td&gt;
  &lt;/tr&gt;
  
  &lt;tr&gt;
    &lt;td style=&quot;width: 164px;&quot;&gt;
      Movement sensor
    &lt;/td&gt;
    
    &lt;td style=&quot;width: 310px;&quot;&gt;
      Detection of personnel
    &lt;/td&gt;
  &lt;/tr&gt;
  
  &lt;tr&gt;
    &lt;td style=&quot;width: 164px;&quot;&gt;
      Arudino Nano
    &lt;/td&gt;
    
    &lt;td style=&quot;width: 310px;&quot;&gt;
      Controlling sensors (System Nano)
    &lt;/td&gt;
  &lt;/tr&gt;
  
  &lt;tr&gt;
    &lt;td style=&quot;width: 164px;&quot;&gt;
      Arduino Nano
    &lt;/td&gt;
    
    &lt;td style=&quot;width: 310px;&quot;&gt;
      Reading system status (HMI Nano)
    &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;the-tools&quot;&gt;The Tools&lt;/h2&gt;

&lt;p&gt;The algorithm was designed using the Arduino IDE in Embedded C.&lt;/p&gt;

&lt;p&gt;This was initial tested using Proteus which simulated the circuitboard.&lt;/p&gt;

&lt;p&gt;We used these &lt;a href=&quot;https://github.com/andresarmento/modbus-arduino&quot;&gt;external libraries&lt;/a&gt; for the configuration of Modbus protocol which was used to help SCADA communicate with micro-controller.&lt;/p&gt;

&lt;p&gt;We implemented the core system logic on the System Nano. Whereas, the SCADA&lt;/p&gt;

&lt;p&gt;system using the Modbus system was implemented on the HMI Nano.&lt;/p&gt;

&lt;p&gt;Using the HMI Nano, we continuously polled the pins of the System Nano to get the status of the devices and sensors.&lt;/p&gt;

&lt;p&gt;Dishonourable mention: Arduino Olimexino is a failed product. It took a lot of troubleshooting attempts before we found out that the microcontroller was sending incorrect signals to the devices (fans changed direction of rotation randomly!)&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;the-code&quot;&gt;The Code&lt;/h2&gt;

&lt;ul style=&quot;list-style-type: square;&quot;&gt;
  &lt;li&gt;
    &lt;a href=&quot;https://github.com/slashr/FireSuppressionIoT/blob/master/FORNANO/FORNANO.ino&quot;&gt;System Nano&lt;/a&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;a href=&quot;https://github.com/slashr/FireSuppressionIoT/blob/master/SCADA_NANO/SCADA_P_V3.ino/SCADA_P_V3.ino.ino&quot;&gt;HMI Nano&lt;/a&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;the-product&quot;&gt;The Product&lt;figure id=&quot;attachment_661&quot; style=&quot;width: 1024px&quot; class=&quot;wp-caption aligncenter&quot;&gt;&lt;/figure&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2018/03/setup.jpg&quot; alt=&quot;Arduino Nano Jr. Arduino Nano Sr., Humble Breadboard, Village Idiot Arduino Olimexino (rightfully abandoned), Whole lotta sensors&quot; /&gt; Starting from the left: Arduino Nano Jr, Arduino Nano Sr, Humble Breadboard, Village Idiot Arduino Olimexino (rightfully abandoned), Whole lotta sensors&lt;/p&gt;</content><author><name>Akash</name></author><category term="Automation" /><category term="arduino" /><category term="IoT" /><category term="microcontroller" /><category term="sensors" /><summary type="html">An IoT experiment to create a gas based suppression system that can be used to extinguish fires inside water sensitive enclosures such as data centers. We make use of Arduino microcontrollers and several sensors to create this system.</summary></entry><entry><title type="html">Using Lambda to Turn Off EC2 Instances During Off Hours</title><link href="https://akashnair.com/using-lambda-turn-off-ec2-instances/" rel="alternate" type="text/html" title="Using Lambda to Turn Off EC2 Instances During Off Hours" /><published>2016-11-24T11:53:30+01:00</published><updated>2016-11-24T11:53:30+01:00</updated><id>https://akashnair.com/using-lambda-turn-off-ec2-instances</id><content type="html" xml:base="https://akashnair.com/using-lambda-turn-off-ec2-instances/">&lt;p&gt;A major part of your AWS bills comes from EC2 uptime and EBS storage use. In order to save on these costs, it is recommended to stop your instances whenever it is not required. Often times, your Development, Test or UAT environments aren’t used by developers during off-hours. Thus this would be a great time frame to temporarily stop the instances and save up on costs.&lt;/p&gt;

&lt;p&gt;In order to do this, we can make use of AWS Lambda, which lets you run your code or without setting up any servers. Since it charges based on the compute time taken up by the code, it is very cost efficient. A task running twice within a 24-hour cycle for typically less than 60 seconds with memory consumption up to 128MB (like the script in this post) typically costs around $0.008 per month.&lt;/p&gt;

&lt;p&gt;We will use the official AWS SDK for Python, boto3 to write our script and then run it using Lambda functions.
&lt;!--more--&gt;&lt;/p&gt;
&lt;hr /&gt;

&lt;h3 id=&quot;pre-requisites&quot;&gt;Pre-requisites&lt;/h3&gt;

&lt;p&gt;First things first, make sure your IAM user has:&lt;/p&gt;

&lt;p&gt;a) Access to AWS Lambda and create functions&lt;/p&gt;

&lt;p&gt;b) Full permissions to create Roles on IAM&lt;/p&gt;

&lt;p&gt;c) Access to CloudWatch&lt;/p&gt;

&lt;p&gt;d) Access to EC2 to tag instances and test the script&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;create-the-function&quot;&gt;Create the Function&lt;/h3&gt;

&lt;p&gt;Once the permissions are in place, get started with the following steps&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;h5 id=&quot;create-an-iam-role&quot;&gt;Create an IAM Role&lt;/h5&gt;

    &lt;p&gt;Create an IAM Role for Lamba to use. You can either create a custom role and give granular permissions. The permissions required are listing, stopping, starting EC2 instances, and listing, creating, modifying CloudWatch logs. Additional dependent permissions may be required so use AWS Policy Simulator to check if you have all the permissions required.&lt;/p&gt;

    &lt;p&gt;Or you can opt for an &lt;strong&gt;AWS Managed Policy&lt;/strong&gt;. Add &lt;strong&gt;AmazonEC2FullAccess&lt;/strong&gt; and &lt;strong&gt;CloudWatchLogsFullAccess&lt;/strong&gt; permissions to the role.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;h5 id=&quot;create-two-lambda-functions-for-starting-and-stopping-the-instances-using-the-two-scripts-below&quot;&gt;Create two Lambda functions, for starting and stopping the instances using the two scripts below.&lt;/h5&gt;

    &lt;ol&gt;
      &lt;li&gt;Refer to &lt;a href=&quot;https://docs.aws.amazon.com/lambda/latest/dg/get-started-create-function.html&quot;&gt;this guide&lt;/a&gt; to get a hang of how to create a simple Lambda function.&lt;/li&gt;
      &lt;li&gt;Click on &lt;strong&gt;Create a function&lt;/strong&gt; and then choose &lt;strong&gt;Author from scratch&lt;/strong&gt;.&lt;/li&gt;
      &lt;li&gt;Enter a suitable name for the function, change the &lt;strong&gt;Runtime&lt;/strong&gt; to Python.&lt;/li&gt;
      &lt;li&gt;Under &lt;strong&gt;Role,&lt;/strong&gt; select &lt;strong&gt;Create a custom role.&lt;/strong&gt; You will be taken to a new page to create the role. Create a policy and then come back to the Lambda page and click &lt;strong&gt;Create Function.&lt;/strong&gt;&lt;/li&gt;
      &lt;li&gt;Under &lt;strong&gt;Add Triggers&lt;/strong&gt;, choose &lt;strong&gt;CloudWatch Events.&lt;/strong&gt; Click on the newly created box to configure it. Under &lt;strong&gt;Schedule expression,&lt;/strong&gt;you can add the cron expression you want.&lt;br /&gt;
   For instance &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cron&amp;lt;strong&amp;gt;(0 3 ? * MON-FRI *)&amp;lt;/strong&amp;gt;&lt;/code&gt;.
        &lt;blockquote&gt;
          &lt;p&gt;Note: As a weird caveat, AWS requires the cron expression to contain a ‘?’ in any one of the first four fields. So &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cron(0 3 * * MON-FRI *)&lt;/code&gt; will not work but &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cron(0 3 ? * MON-FRI *)&lt;/code&gt; will.&lt;/p&gt;
        &lt;/blockquote&gt;
      &lt;/li&gt;
      &lt;li&gt;Then click on the function box and copy paste the above code into the editor window.&lt;/li&gt;
      &lt;li&gt;Choose the &lt;strong&gt;No VPC&lt;/strong&gt; option under &lt;strong&gt;Network&lt;/strong&gt;. Keep your Lambda function inside a VPC when the function needs to login into an EC2 instance which is kept in a private subnet and is not publicly accessible. For actions that can be performed over the internet, it is recommended to not put the function inside a VPC. For example, boto’s start and stop API calls are publicly exposed and accessible over the internet which means that Lambda invoke them over the internet but not from within a VPC&lt;/li&gt;
      &lt;li&gt;Click on &lt;strong&gt;Save&lt;/strong&gt; and then click on &lt;strong&gt;Test&lt;/strong&gt; for a dry run.&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;You might want to run a few tests to ensure everything is working as expected. You don’t want your instance to be automatically shut down at 15:00 UTC (or maybe you do)&lt;/p&gt;</content><author><name>Akash</name></author><category term="Automation" /><category term="AWS" /><category term="Ops" /><category term="automation" /><category term="cloudwatch" /><category term="devops" /><category term="ec2" /><summary type="html">A major part of your AWS bills comes from EC2 uptime and EBS storage use. In order to save on these costs, it is recommended to stop your instances whenever it is not required. Often times, your Development, Test or UAT environments aren’t used by developers during off-hours. Thus this would be a great time frame to temporarily stop the instances and save up on costs. In order to do this, we can make use of AWS Lambda, which lets you run your code or without setting up any servers. Since it charges based on the compute time taken up by the code, it is very cost efficient. A task running twice within a 24-hour cycle for typically less than 60 seconds with memory consumption up to 128MB (like the script in this post) typically costs around $0.008 per month. We will use the official AWS SDK for Python, boto3 to write our script and then run it using Lambda functions.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://akashnair.com/wp-content/uploads/2016/11/aws-blackbelt-2015-aws-lambda-8-638.jpg" /><media:content medium="image" url="https://akashnair.com/wp-content/uploads/2016/11/aws-blackbelt-2015-aws-lambda-8-638.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">RDS Read Replica for Aurora using MySQL</title><link href="https://akashnair.com/rds-read-replica-aurora-mysql/" rel="alternate" type="text/html" title="RDS Read Replica for Aurora using MySQL" /><published>2016-09-07T11:03:15+02:00</published><updated>2016-09-07T11:03:15+02:00</updated><id>https://akashnair.com/rds-read-replica-aurora-mysql</id><content type="html" xml:base="https://akashnair.com/rds-read-replica-aurora-mysql/">&lt;p&gt;Default Read Replicas provided by AWS are instances which should be of the same instance type as the master DB or higher. This might not always be an ideal solution since read replica servers do not require significant computing power. As an alternative, one can create a read replica on a standalone EC2 instance or a RDS DB instance. In the following scenario, we’ll create a read replica running on MySQL syncing with a master DB running on Amazon Aurora. The replica instance type can be any type as seen fit by the user.
&lt;!--more--&gt;&lt;/p&gt;
&lt;hr /&gt;

&lt;h3 id=&quot;configure-master-db-for-replication&quot;&gt;Configure Master DB for replication&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;h4 id=&quot;enable-binary-logging-on-master&quot;&gt;Enable Binary Logging on master&lt;/h4&gt;

    &lt;ul&gt;
      &lt;li&gt;On the RDS Dashboard, click on &lt;strong&gt;Parameter Groups&lt;/strong&gt; on the left and then click on &lt;strong&gt;Create Parameter Group&lt;/strong&gt;. Under &lt;strong&gt;Type,&lt;/strong&gt; select DB Cluster Parameter Group. Enter a relevant &lt;strong&gt;Group Name&lt;/strong&gt; and &lt;strong&gt;Description&lt;/strong&gt; and click &lt;strong&gt;Create&lt;/strong&gt;.&lt;/li&gt;
      &lt;li&gt;Next, select the Parameter Group just created and click on &lt;strong&gt;Edit Parameters.&lt;/strong&gt; Then, under &lt;strong&gt;binlog_format&lt;/strong&gt;, change from &lt;strong&gt;OFF&lt;/strong&gt; to &lt;strong&gt;MIXED.&lt;/strong&gt;Click &lt;strong&gt;Save Changes&lt;/strong&gt;&lt;/li&gt;
      &lt;li&gt;Click on &lt;strong&gt;Clusters&lt;/strong&gt; on the RDS Dashboard, select the (master DB) cluster and click on &lt;strong&gt;Modify Cluster.&lt;/strong&gt; Under DB Cluster Parameter Group, select the Parameter Group just created. Click on &lt;strong&gt;Continue&lt;/strong&gt; and then on &lt;strong&gt;Modify Cluster.&lt;/strong&gt; The instance may reboot or become briefly inaccessible.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;h4 id=&quot;retain-binary-logs-set-retention-period&quot;&gt;Retain binary logs. Set retention period&lt;/h4&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;To do this, login to the master DB and run the following command:&lt;/p&gt;

        &lt;p&gt;&lt;code&gt; CALL mysql.rds_set_configuration(&apos;binlog retention hours&apos;, 144); &lt;/code&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Run &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;SHOW BINARY LOGS&lt;/code&gt; to ensure that logs are being retained.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;h4 id=&quot;create-snapshot-of-master-and-restore-from-snapshot&quot;&gt;Create snapshot of master and restore from snapshot.&lt;/h4&gt;
    &lt;ul&gt;
      &lt;li&gt;Create a snapshot of the master DB cluster. Restore from the snapshot a new RDS instance having the same parameter group as the master DB cluster (so that binary logging is enabled).&lt;/li&gt;
      &lt;li&gt;Connect to the newly created cluster and run &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;SHOW MASTER STATUS&lt;/code&gt; command. Retrieve the current binary log file name from the &lt;code&gt;File&lt;/code&gt; field and the log file position from the&lt;code&gt;Position&lt;/code&gt; field. Save these values for when you start replication.&lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Create a dump of the database. Use the following command:&lt;/p&gt;

        &lt;p&gt;&lt;code&gt; mysqldump -databases (database-name) -single-transaction -order-by-primary -r backup.sql -u (database-username) -host=(database-address) -p &lt;/code&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;After the dump is created, delete the restored DB cluster.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;configure-slave-db-for-replication&quot;&gt;Configure Slave DB for replication&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;Launch a MySQL RDS instance.&lt;/li&gt;
  &lt;li&gt;Connect to this MySQL RDS instance (slave DB) and restore the dump (source backup.sql)&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Enable replication by running the following queries (given in the &lt;a href=&quot;https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/AuroraMySQL.Replication.MySQL.html&quot;&gt;AWS documentation&lt;/a&gt;) on the master DB.&lt;/p&gt;

    &lt;p&gt;&lt;code&gt; 
        CALL mysql.rds_set_external_master (&quot;(master-db-endpoint)&quot;, 3306,&quot;(username)&quot;, &quot;(password)&quot;, &quot;(binary-log-file-name-from-step-3)&quot;, (log-file-position-from-step-3), 0); 
        CALL mysql.rds_start_replication;
        CALL mysql.rds_start_replication;
 &lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Finally, run SHOW SLAVE STATUS command on the replica and check &lt;strong&gt;Seconds behind master.&lt;/strong&gt; If the value is 0, it means there is no replica lag. When the replica lag is 0, reduce the retention period by setting the parameter binlog &lt;em&gt;retention hours&lt;/em&gt; to a smaller time frame.&lt;/li&gt;
&lt;/ol&gt;</content><author><name>Akash</name></author><category term="AWS" /><category term="Ops" /><category term="aurora" /><category term="aws" /><category term="devops" /><category term="mysql" /><category term="rds" /><summary type="html">Default Read Replicas provided by AWS are instances which should be of the same instance type as the master DB or higher. This might not always be an ideal solution since read replica servers do not require significant computing power. As an alternative, one can create a read replica on a standalone EC2 instance or a RDS DB instance. In the following scenario, we’ll create a read replica running on MySQL syncing with a master DB running on Amazon Aurora. The replica instance type can be any type as seen fit by the user.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://akashnair.com/wp-content/uploads/2016/09/2000px-AWS_Simple_Icons_Database_Amazon_RDS_DB_Instance_Read_Replica.svg_.png" /><media:content medium="image" url="https://akashnair.com/wp-content/uploads/2016/09/2000px-AWS_Simple_Icons_Database_Amazon_RDS_DB_Instance_Read_Replica.svg_.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Docker: Install, Build an Image and Publish to DockerHub</title><link href="https://akashnair.com/docker/" rel="alternate" type="text/html" title="Docker: Install, Build an Image and Publish to DockerHub" /><published>2016-07-20T13:34:36+02:00</published><updated>2016-07-20T13:34:36+02:00</updated><id>https://akashnair.com/docker</id><content type="html" xml:base="https://akashnair.com/docker/">&lt;p&gt;Docker is a &lt;a href=&quot;https://en.wikipedia.org/wiki/Operating-system-level_virtualization&quot;&gt;container technology&lt;/a&gt; that provides a &lt;strong&gt;layer of abstraction&lt;/strong&gt; on the OS that it is installed on(Linux or Windows). It allows the packaging of a software application into a &lt;strong&gt;image&lt;/strong&gt; which consists of everything the software requires to run: code, system utilities, third party libraries and other dependencies. Then this image is run on the common &lt;strong&gt;layer of abstraction&lt;/strong&gt; defined above. Therefore, it becomes easy to move and deploy the software application (which refers to the &lt;strong&gt;container&lt;/strong&gt;) across any infrastructure or environment.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Updated: August, 2018&lt;/em&gt;&lt;/strong&gt;
&lt;!--more--&gt;&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;pre-requisites&quot;&gt;Pre-requisites:&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;Ubuntu 16.04 LTS 64-bit version. (Docker does not run on 32 bit)&lt;/li&gt;
  &lt;li&gt;Kernel version 3.10 and above. Check current kernel version using uname -r&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h4 id=&quot;installation&quot;&gt;Installation&lt;/h4&gt;

&lt;h5 id=&quot;the-easy-way&quot;&gt;The Easy Way&lt;/h5&gt;

&lt;p&gt;&lt;code&gt;wget -qO- https://get.docker.com/ | sh&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;OR&lt;/p&gt;

&lt;h5 id=&quot;the-longer-way-in-case-the-easy-way-doesnt-work-out&quot;&gt;The Longer Way (in case the easy way doesn’t work out)&lt;/h5&gt;

&lt;ol&gt;
  &lt;li&gt;Update APT sources &amp;amp; enable APT to work with HTTPS:
    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;code&gt;apt-get update&lt;/code&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;code&gt;sudo apt-get install apt-transport-https ca-certificates curl software-properties-common&lt;/code&gt;&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Add the GPG keys:
    &lt;ul&gt;
      &lt;li&gt;&lt;code&gt; curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - &lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Add the stable repository to the apt sources list:
    &lt;ul&gt;
      &lt;li&gt;&lt;code&gt;sudo add-apt-repository \   
 &quot;deb [arch=amd64] https://download.docker.com/linux/ubuntu \
 $(lsb_release -cs) \
 stable&quot;&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Update apt resource list:
    &lt;ul&gt;
      &lt;li&gt;&lt;code&gt;apt-get update&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Install Docker Community Edition
    &lt;ul&gt;
      &lt;li&gt;&lt;code&gt;apt-get install docker-ce&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;hr /&gt;

&lt;h4 id=&quot;running-docker&quot;&gt;Running Docker&lt;/h4&gt;

&lt;ol&gt;
  &lt;li&gt;Start Docker and test if it installed correctly. The second command will download the “hello-world” image form DockerHub, create a container and run it.
    &lt;ul&gt;
      &lt;li&gt;&lt;code&gt;service docker start&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;docker run hello-world&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;To view all the containers on the system, type:
    &lt;ul&gt;
      &lt;li&gt;&lt;code&gt;docker ps -a&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Try downloading and running the whalesay image from DockerHub. In order to do this, first create an account on &lt;a href=&quot;https://hub.docker.com/&quot;&gt;DockerHub&lt;/a&gt; and search for the image docker/whalesay. On the image page, you’ll find instructions on usage. For whalesay, use the following command
    &lt;ul&gt;
      &lt;li&gt;&lt;code&gt;docker run docker/whalesay cowsay boo&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;After the image downloads and runs, you’ll see the input in the form of a ASCII whale with a “boo” caption. Not very profound, I know. You can play around with the image by passing different parameters. Say:
    &lt;ul&gt;
      &lt;li&gt;&lt;code&gt;docker run docker/whalesay cowsay skywide!&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;In order to view all the images on the system, type:
    &lt;ul&gt;
      &lt;li&gt;&lt;code&gt;docker images&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;We’ll next learn how to build your own images. For now, use the above whalesay image and modify it to create our new image&lt;/li&gt;
&lt;/ol&gt;

&lt;hr /&gt;

&lt;h4 id=&quot;creating-your-own-docker-image&quot;&gt;Creating your own Docker image&lt;/h4&gt;

&lt;ol&gt;
  &lt;li&gt;Create a directory skywide.&lt;/li&gt;
  &lt;li&gt;CD into the directory&lt;/li&gt;
  &lt;li&gt;Create a file Dockerfile
    &lt;ul&gt;
      &lt;li&gt;&lt;code&gt;touch Dockerfile&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Open Dockerfile in an editor and add the following.:
    &lt;ul&gt;
      &lt;li&gt;&lt;code&gt;FROM: docker/whalesay:latest&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;RUN: apt-get -y update &amp;amp;&amp;amp; apt-get install -y fortunes&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;CMD: /usr/games/fortune -a | cowsay &lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;The FROM command tells Docker which image your image is based on. RUN command tells Docker to install the required programs/libraries on your image. CMD tells Docker to run the specified command when the image is run inside a container.&lt;/li&gt;
  &lt;li&gt;Next, build your image by running:
    &lt;ul&gt;
      &lt;li&gt;&lt;code&gt;docker build -t docker-whale .&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;The docker build command requires a Dockerfile to be present in the current directory. The “-t” option gives a name to the image being built (in this case, it is “docker-whale”)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Docker will build a new image by using the whalesay image that it downloaded earlier and on this new image it will update the apt-get cache and install the software fortunes.&lt;/li&gt;
  &lt;li&gt;You can then verify that a new image has been built by running docker images. You should now see three images including “docker-whale”.&lt;/li&gt;
  &lt;li&gt;Finally, see your own image in action! Go ahead and run:
    &lt;ul&gt;
      &lt;li&gt;&lt;code&gt;docker run docker-whale&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;hr /&gt;

&lt;h4 id=&quot;pushing-your-image-to-dockerhub&quot;&gt;Pushing your image to DockerHub&lt;/h4&gt;

&lt;ol&gt;
  &lt;li&gt;Create a new repository to upload your docker image to. Make sure the repository is public&lt;/li&gt;
  &lt;li&gt;Note down the image ID from
    &lt;ul&gt;
      &lt;li&gt;&lt;code&gt;docker images&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Tag your image with your username
    &lt;ul&gt;
      &lt;li&gt;&lt;code&gt;docker tag (image-id) (dockerhub-username) (repository-name):latest&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Type docker images again to see your new tagged image&lt;/li&gt;
  &lt;li&gt;You will need to login to Docker from the terminal before you can push it. To login
    &lt;ul&gt;
      &lt;li&gt;&lt;code&gt;docker login --username=(username) --email=(docker-email)&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;To push the image to your repo, type
    &lt;ul&gt;
      &lt;li&gt;&lt;code&gt;docker push (username)/(image-name)&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Finally, after the command finishes running, go to your DockerHub repository page to find your uploaded image.&lt;/li&gt;
&lt;/ol&gt;</content><author><name>Akash</name></author><category term="Automation" /><category term="Ops" /><category term="automation" /><category term="container" /><category term="devops" /><category term="docker" /><summary type="html">Docker is a container technology that provides a layer of abstraction on the OS that it is installed on(Linux or Windows). It allows the packaging of a software application into a image which consists of everything the software requires to run: code, system utilities, third party libraries and other dependencies. Then this image is run on the common layer of abstraction defined above. Therefore, it becomes easy to move and deploy the software application (which refers to the container) across any infrastructure or environment. Updated: August, 2018</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://akashnair.com/wp-content/uploads/2016/07/large_v-trans.png" /><media:content medium="image" url="https://akashnair.com/wp-content/uploads/2016/07/large_v-trans.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Ansible Playbook to Create a Load-Balanced Tomcat Cluster</title><link href="https://akashnair.com/ansible-playbook-tomcat-cluster/" rel="alternate" type="text/html" title="Ansible Playbook to Create a Load-Balanced Tomcat Cluster" /><published>2016-07-13T09:35:26+02:00</published><updated>2016-07-13T09:35:26+02:00</updated><id>https://akashnair.com/ansible-playbook-tomcat-cluster</id><content type="html" xml:base="https://akashnair.com/ansible-playbook-tomcat-cluster/">&lt;p&gt;This Ansible playbook should give an idea about the configuration management capabilities of Ansible. It sets up a group of Tomcat nodes which are load-balanced by Apache using the Mod-jk module. In addition to this, it also sets up all other dependencies and also configures the installed services to run on startup.
&lt;!--more--&gt;
Overview of actions performed:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Update apt-get cache&lt;/li&gt;
  &lt;li&gt;Install Apache&lt;/li&gt;
  &lt;li&gt;Install Mod-jk module&lt;/li&gt;
  &lt;li&gt;Install MySQL&lt;/li&gt;
  &lt;li&gt;Install Java&lt;/li&gt;
  &lt;li&gt;Download Tomcat and setup nodes&lt;/li&gt;
  &lt;li&gt;Edit and configures Tomcat configuration files&lt;/li&gt;
  &lt;li&gt;Edit and configures Apache and Mod-jk configuration files&lt;/li&gt;
  &lt;li&gt;Restart Apache&lt;/li&gt;
  &lt;li&gt;Start Tomcat nodes&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Find the playbook in the repo here:
&lt;a href=&quot;https://github.com/slashr/Tomcat-Cluster-Load-Balancing&quot;&gt;https://github.com/slashr/Tomcat-Cluster-Load-Balancing&lt;/a&gt;&lt;/p&gt;</content><author><name>Akash</name></author><category term="Automation" /><category term="AWS" /><category term="ansible" /><category term="automation" /><category term="aws" /><category term="devops" /><category term="ec2" /><summary type="html">This Ansible playbook should give an idea about the configuration management capabilities of Ansible. It sets up a group of Tomcat nodes which are load-balanced by Apache using the Mod-jk module. In addition to this, it also sets up all other dependencies and also configures the installed services to run on startup.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://akashnair.com/wp-content/uploads/2016/05/Ansible-Official-Logo-Black.png" /><media:content medium="image" url="https://akashnair.com/wp-content/uploads/2016/05/Ansible-Official-Logo-Black.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">IAM: Restrict EC2 Access to Specific Region</title><link href="https://akashnair.com/iam-restrict-ec2-access-specific-region/" rel="alternate" type="text/html" title="IAM: Restrict EC2 Access to Specific Region" /><published>2016-07-06T13:55:30+02:00</published><updated>2016-07-06T13:55:30+02:00</updated><id>https://akashnair.com/iam-restrict-ec2-access-specific-region</id><content type="html" xml:base="https://akashnair.com/iam-restrict-ec2-access-specific-region/">&lt;p&gt;Restrict user activity to a specific region in order to minimize resource wastage. This policy will ensure that all of the resources are being created only in the region of your choice so that when it comes down to resource cleanup and housekeeping, you’ll be able to save quite a lot of time.&lt;/p&gt;

&lt;p&gt;Apart from restricting access region-wise, it allows users read-only access to resources launched in other regions.&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/slashr/f17eb97ac86e94160ce03cc920373143.js&quot;&gt;&lt;/script&gt;</content><author><name>Akash</name></author><category term="AWS" /><category term="Custom IAM Policies" /><category term="aws" /><category term="ec2" /><category term="iam" /><summary type="html">Restrict user activity to a specific region in order to minimize resource wastage. This policy will ensure that all of the resources are being created only in the region of your choice so that when it comes down to resource cleanup and housekeeping, you’ll be able to save quite a lot of time. Apart from restricting access region-wise, it allows users read-only access to resources launched in other regions.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://akashnair.com/wp-content/uploads/2016/07/iam-logo.png" /><media:content medium="image" url="https://akashnair.com/wp-content/uploads/2016/07/iam-logo.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>